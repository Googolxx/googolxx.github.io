<!DOCTYPE html>
<html lang="zh-cn">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Renjie&#39;s log</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="">
    <meta name="generator" content="Hugo 0.146.5">
    
    
    
      <meta name="robots" content="noindex, nofollow">
    
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



  


    


    
      

    

    
    
      <link href="/index.xml" rel="alternate" type="application/rss+xml" title="Renjie&#39;s log" />
      <link href="/index.xml" rel="feed" type="application/rss+xml" title="Renjie&#39;s log" />
      
    

    
      <link rel="canonical" href="http://localhost:1313/">
    

    <meta property="og:url" content="http://localhost:1313/">
  <meta property="og:site_name" content="Renjie&#39;s log">
  <meta property="og:title" content="Renjie&#39;s log">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="website">

  <meta itemprop="name" content="Renjie&#39;s log">
  <meta itemprop="datePublished" content="2025-06-22T13:43:59+08:00">
  <meta itemprop="dateModified" content="2025-06-22T13:43:59+08:00">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Renjie&#39;s log">

	




<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  window.MathJax  = {
    tex: {
        inlineMath: [['$', '$'], ['\$', '\$']], 
        displayMath: [['$$', '$$']],
        processEnvironments: true,
        
        packages: ['base', 'ams', 'noerrors', 'noundefined'],
        tags: "ams",
    },
    loader:{
        load: ['ui/safe', '[tex]/ams'], 
    
    },
  };
</script>

<link rel="stylesheet" href="/css/custom.css">


  </head><body class="ma0 avenir bg-near-white development">

    

  <header>
    <div class="pb3-m pb6-l bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l center items-center justify-between">
    <a href="/" class="f3 fw2 hover-white white-90 dib no-underline">
      
        Renjie&#39;s log
      
    </a>
    <div class="flex-l items-center">
      

      
      <div class="ananke-socials"></div>

    </div>
  </div>
</nav>

      <div class="tc-l pv3 ph3 ph4-ns">
        <h1 class="f2 f-subheadline-l fw2 light-silver mb0 lh-title">
          Renjie&#39;s log
        </h1>
        
      </div>
    </div>
  </header>


    <main class="pb7" role="main">
      
  <article class="cf ph3 ph5-l pv3 pv4-l f4 tc-l center measure-wide lh-copy nested-links mid-gray">
    
  </article>

  
  
  
  
  

  
    <div class="pa3 pa4-ns w-100 w-70-ns center">
      

      <section class="w-100 mw8">
        
        
          <div class="w-100 mb4 relative">
            
<article class="bb b--black-10">
  <div class="db pv4 ph3 ph0-l dark-gray no-underline">
    <div class="flex-column flex-row-ns flex">
      
      <div class="blah w-100">
        <h1 class="f3 fw1 athelas mt0 lh-title">
          <a href="/posts/diffusion-based-generative-models-1/" class="color-inherit dim link">
            Diffusion-Based Generative Models &lt;1&gt;: DDPM
            </a>
        </h1>
        <div class="f6 f5-l lh-copy nested-copy-line-height nested-links">
          <!-- summary = "深入解读去噪扩散概率模型 (DDPM) 的核心算法与数学推导，揭示其如何通过前向加噪与反向去噪过程实现高质量生成。" -->
<h2 id="一引言">一、引言</h2>
<p>扩散模型（<strong>Diffusion Models</strong>）作为当前生成式 AI 的核心范式，受热力学启发<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>，主要思想是迭代地加噪/去噪数据，模拟粒子扩散过程。在图像、视频生成等领域实现了非常好的效果。下文介绍核心代表作之一 <strong>DDPM</strong> <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> (Denoising Diffusion Probabilistic Models)。</p>
<p>随着 Diffusion-Based Generative Models 理论的逐渐完善，可以从多种视角（分数匹配、 微分方程等）推导出 DDPM 的前向/逆向扩散过程、优化目标和采样过程。这里，我们将遵循 DDPM 原文的思路进行推导。</p>
<h2 id="二ddpm-算法框架">二、DDPM 算法框架</h2>
<h3 id="1-前向扩散过程forward-diffusion-process">1. 前向扩散过程（Forward Diffusion Process）</h3>
<!-- 前向扩散过程是***无参***的扩散过程，服从一个马尔可夫链 (Markov Chain)：马尔科夫链为状态空间中经过从一个状态到另一个状态的转换的随机过程，该过程要求具备“无记忆性 ”，即下一状态的概率分布只能由当前状态决定，在时间序列中它前面的事件均与之无关。

具体来说，从一个真实数据分布采样 $\mathbf{x}_0 \sim q(\mathbf{x})$, 通过逐步对数据 $x_0$ 添加高斯噪声（Gaussian Noise），得到被扰动的样本 $ x_1,...x_t,...x_T $，在 $T$ 步后接近纯噪声。得益于高斯分布的特殊数学性质，其线性组合仍然是高斯分布，因此可以将加噪过程中互相独立的高斯噪声进行合并:


$$
\begin{aligned}
x_t &= \sqrt{\alpha_t} x_{t-1} + \sqrt{1 - \alpha_t} \epsilon_{t-1} \\
    &= \sqrt{\alpha_t} \left( \sqrt{\alpha_{t-1}} x_{t-2} + \sqrt{1 - \alpha_{t-1}} \epsilon_{t-2} \right) + \sqrt{1 - \alpha_t} \epsilon_{t-1} \\
    % &= \sqrt{\alpha_t \alpha_{t-1}} x_{t-2} + \sqrt{\alpha_t} \sqrt{1 - \alpha_{t-1}} \epsilon_{t-2} + \sqrt{1 - \alpha_t} \epsilon_{t-1} \\
    &= \sqrt{\alpha_{t-1} \alpha_t } x_{t-2} + \underbrace{{\sqrt{\alpha_t} \sqrt{1 - \alpha_{t-1}} \epsilon_{t-2} + \sqrt{1 - \alpha_t} \epsilon_{t-1}}}_{\text{Combine noise using linear Gaussian}} \\
    &= \sqrt{\alpha_{t-1} \alpha_t} x_{t-2} + \sqrt{1 - \alpha_{t-1} \alpha_t} \bar{\epsilon}_{t-2} \\
    &=  ... \\
    &= \sqrt{\bar{\alpha_t}} x_0 +  \sqrt{1 - \bar{\alpha_t}} \bar{\epsilon}_{t}
\end{aligned}
$$

其中 $ \{ \alpha_0, \dots, \alpha_T \}$ 是一组人为设置的超参数，用于控制扩散过程中噪声的强度， 定义 $\bar{\alpha_t} = \prod_{i=1}^t \alpha_i$。那么我们可以得到:

$$
\begin{aligned}
q(x_t|x_{t-1}) &\sim \mathcal{N}(x_t | \sqrt{{\alpha_t}} x_{t-1}, (1 - \alpha_t) I) \\
q(x_t|x_0) &\sim \mathcal{N}(x_t | \sqrt{\bar{\alpha_t}} x_0, (1 - \bar{\alpha_t}) I)
\end{aligned}
$$

当扩散过程足够长，可以得到预先假设的先验分布 $ q(x_T) \sim \mathcal{N}(x_T |0, I)$

Note：
- 加噪过程中设置系数为 $ \sqrt{\alpha_t} 和 (1 - \sqrt{\alpha_t}) $ 是使其平方和为 $1$, 从而保持扩散过程中方差的稳定。在 SMLD 中, 因为系数设置的不同, 为方差膨胀的形式。
- 逆向分布 $q(x_{t-1}|x_{t})$ 没有显式解析解，因为 $ x_{t-1} 和 \epsilon_{t}$ 的依赖性使得无法直接利用前向过程的线性高斯性质。但是，当 $ 1-\alpha_t$ 足够小（即扩散步长极短或总步数 $T$ 足够大）时，$q(x_{t-1}|x_{t})$ 可近似为高斯分布，这一近似在扩散过程的连续极限下（如随机微分方程SDE的视角下）有理论支持。


### 2. 逆向扩散过程（Reverse Diffusion Process）
前向过程在手动设计下，均有明确的解析解。在假设逆向过程也为马尔可夫链的情况下，如果我们能够得到逆向过程 $q(x_{t-1}|x_{t})$ 的形式，那就能够根据联合分布 $ q(x_0, x_1,..., x_T)$ ，从先验分布 $ q(x_T) \sim \mathcal{N}(x_T |0, I)$ 开始，逐步采样得到 $x_0$ 。上文提到 $q(x_{t-1}|x_{t})$ 虽然是未知的，但可近似为高斯分布，所以这里用参数化的神经网络学习 $p_{\theta}(x_{t-1}|x_t)$ 来逼近 $q(x_{t-1}|x_{t})$ :

$$
p_\theta(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t,t), \Sigma_\theta(x_t,t))
$$

完成训练后得到近似真实逆向分布的 $p_{\theta}(x_{t-1}|x_t)$，即可通过联合分布来进行采样：

$$
p_{\theta}(x_0,x_1,...,x_T) = p(x_T) \prod_{t=1}^{T} p_{\theta}(x_{t-1}|x_t)
$$

定义了前向和逆向的扩散过程，下一步就是确定优化目标，也就是损失函数。 -->
<p>前向扩散过程是<em><strong>无参</strong></em>的扩散过程，服从一个马尔可夫链 (Markov Chain)：马尔科夫链为状态空间中经过从一个状态到另一个状态的转换的随机过程，该过程要求具备&quot;无记忆性&quot;，即下一状态的概率分布只能由当前状态决定，在时间序列中它前面的事件均与之无关。</p>
        </div>
        <a href="/posts/diffusion-based-generative-models-1/" class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a>
      </div>
    </div>
  </div>
</article>

          </div>
        
          <div class="w-100 mb4 relative">
            
<article class="bb b--black-10">
  <div class="db pv4 ph3 ph0-l dark-gray no-underline">
    <div class="flex-column flex-row-ns flex">
      
      <div class="blah w-100">
        <h1 class="f3 fw1 athelas mt0 lh-title">
          <a href="/posts/test/" class="color-inherit dim link">
            Test
            </a>
        </h1>
        <div class="f6 f5-l lh-copy nested-copy-line-height nested-links">
          <h3 id="正文">正文</h3>
<p>注意这里可能会有歧义，</p>
$$
\begin{equation}
\begin{aligned}
p_{\theta}(x) &= \frac{p_{\theta}(X, z)}{p(z|X)} \\
    &= \frac{P_{\theta}(X|z)p(z)}{p(z|X)}
\end{aligned}
\end{equation}
$$<p>(注意这里可能会有歧义，我用 $p(z|X)$替代了 $p_{\theta}(z|X)$，但最后都会优化掉)。因为 $ p(z) \sim \mathcal{N}(z|0, I) $，而 $p_{\theta}(X|z)$ 就是Decoder所生成的分布，如果知道真实后验分布 $p(z|x)$，那我们也可以直接优化目标函数。但核心 $p(z|x)$ 是untracble的 （当然更严谨一点讲，也可以用hybird MC等方式来逼近，但就不在这里的讨论范畴了）。</p>
<p>于是在VAE中，我们可以用变分贝叶斯，引入一个Encoder，生成 $ q_{\phi}(z|X) \sim \mathcal{N}(z|\mu(X;\phi), \sigma(X;\phi)I)$ 来逼近真实后验分布 $p(z|X)$ （类似地，这里协方差矩阵也为对角矩阵）。重新推导目标函数</p>
$$
\begin{equation}
\begin{aligned}
\log p_{\theta}(X) &\approx \frac{1}{m} \sum_{i=0}^{m} p_{\theta}(X|z_{i}) \\
    &= \frac{1}{m} \sum_{i=0}^{m} \log \left( \frac{1}{(2\pi)^{K/2} |\Sigma|^{1/2}} \exp\left(-\frac{1}{2} (X - f(z_i))^T \Sigma^{-1} (X - f(z_i))\right) \right) \\
    &= \frac{1}{m} \sum_{i=0}^{m} \left[ -\frac{K}{2} \log(2\pi) - \frac{1}{2} \log |\Sigma| - \frac{1}{2} (X - f(z_i))^T \Sigma^{-1} (X - f(z_i)) \right] \\
    &\propto \frac{1}{m} \sum_{i=0}^{m} - \frac{1}{2} (X - f(z_i))^T \Sigma^{-1} (X - f(z_i)) \\ 
    &\propto \frac{1}{m} \sum_{i=0}^{m} \sum_{k=0}^{K} \frac{\left( x^{(k)} - f(z_i;\theta)^{(k)} \right)^{2}}{\sigma^{(k)}} \\
    &\propto \frac{1}{m} \sum_{i=0}^{m} \sum_{k=0}^{K} \left( x^{(k)} - f(z_i;\theta)^{(k)} \right)^{2}
\end{aligned}
\end{equation}
$$<p>因为真实后验分布 $p(z|X)$ 没有解析解，且KL散度这一项 $ D_{\text{KL}}(q_{\phi}(z|X) | p(z|X)) $ 始终是大于0的，因此目标优化函数可以改为最大化 第一项。在变分贝叶斯方法中，这个损失函数被称为<strong>变分下界或证据下界（variational lower bound, or evidence lower bound）</strong></p>
        </div>
        <a href="/posts/test/" class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a>
      </div>
    </div>
  </div>
</article>

          </div>
        
          <div class="w-100 mb4 relative">
            
<article class="bb b--black-10">
  <div class="db pv4 ph3 ph0-l dark-gray no-underline">
    <div class="flex-column flex-row-ns flex">
      
      <div class="blah w-100">
        <h1 class="f3 fw1 athelas mt0 lh-title">
          <a href="/posts/variational-autoencoder/" class="color-inherit dim link">
            Variational Autoencoder
            </a>
        </h1>
        <div class="f6 f5-l lh-copy nested-copy-line-height nested-links">
          <h2 id="生成模型的目标">生成模型的目标</h2>
<p>生成模型（Generative Models）的目的是想学习真实数据分布 $p(x)$， 其中 $X$ 通常是定义在某个（高维）空间 $\mathcal{X}$ 上的数据点。比如一张图像就是一个高维数据点，每个像素对应一个维度。具体来讲生成模型想要解决的问题：<em><strong>考虑一个从真实分布 $p(x)$ 中采样得到的数据集 $ \lbrace{x_1, x_2, \dots, x_n \rbrace}$  ，我们希望从采样得到的数据子集中学习一个分布 $p_\theta(x)$ ，逼近真实分布 $p(x)$</strong></em>。</p>
<h2 id="变分自编码器-variational-autoencoder">变分自编码器 Variational Autoencoder</h2>
<p>变分自编码器（VAE）作为一种生成模型，依然在现在的机器学习算法占有一席之地。VAE的优化目标推导其实有好几种方式，在开始之前，我想先从最简单的例子开始。</p>
<h3 id="简单假设下存在的问题">简单假设下存在的问题</h3>
<p>考虑对人脸数据集CelebA的建模，我们希望从CelebA数据集中学习到分布 $p_\theta(x)$，然后从 $p_\theta(x)$ 中采样得到新的人脸样本。从流形假设（Manifold Hypothesis）的角度来讲，自然图像数据在高维像素空间中形成一个稠密子集，其内在结构可以用一个低维、非线性流形来近似建模；或者说，图像数据服从一个 <em><strong>嵌入在高维像素空间中低维非线性流形分布</strong></em> 。以CelebA为例，每张图像的数据维度为178x218x3维，RGB图像每一维有256种取值，这个一个非常庞大的高维空间，只有极少数组合才对应一张“真实的人脸”，实际上影响人脸的因素可以抽象为具体几类（比如表情，年龄，肤色，五官轮廓等等）。当然，具体抽象成哪些类别并不是我们关心的问题，我们关心的是高维（图像）数据 $x$ 到低维空间隐变量 $z$（latent variables）的映射关系，通过构建这对映射关系，我们能够实现从 $p(z)$ 中采样，生成新样本 $ \hat{x}$。其实深度学习中不少领域都与该流形假设有关，比如自编码器、表示学习、对抗样本等。</p>
<p>基于上面的想法，一个很自然的想法浮现在脑海中：可以直接构建一个解码器（Decoder），从先验分布 $p(z)$ 中采样，作为Decoder的输入，生成样本并和真实分布中的数据求距离：</p>
$$
\begin{equation}
\begin{aligned}
p_{\theta}(X) &= \int p_{\theta}(X|z)p(z) dz \\
     &= \int \mathcal{N}(X|f(z;\theta), \Sigma) \cdot \mathcal{N}(z|0, I) dz \\
     &= \mathbb{E}_{z \sim p(z)} \left[ p_{\theta}(X|z) \right] \\
     &\approx \frac{1}{m} \sum_{i=0}^{m} p_{\theta}(X|z_{i})
\end{aligned}
\end{equation}
$$<p>其中，$f(z;\theta)$ 是隐变量 $z$ 到样本空间 $ X$ 的映射函数，在这里也就是Decoder，隐变量 $z$ 通常假设为服从均值为 $0$，协方差矩阵为单元矩阵 $I$ 的高斯分布 $\mathcal{N}(z|0, I) $；Decoder生成的样本分布 $p_{\theta}(X|z)$ 的均值，协方差矩阵 $\Sigma$ 一般设为常数。容易发现，我们利用蒙特卡洛采样（Monte Carlo Sampling）从 $p(z)$ 中采样，经过Decoder就可以生成新的样本了，然后计算损失，反向传播优化Decoder了。
<figure><img src="/pic_vae/dec.png" width="1600">
</figure>
</p>
        </div>
        <a href="/posts/variational-autoencoder/" class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a>
      </div>
    </div>
  </div>
</article>

          </div>
        
      </section>

      

    </div>
  

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white white-70 dn dib-ns pv2 ph3 no-underline" href="http://localhost:1313/" >
    &copy;  Renjie's log 2025 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

  </body>
</html>
