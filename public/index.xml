<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Renjie Zou blog list</title>
    <link>https://example.org/</link>
    <description>Recent content on Renjie Zou blog list</description>
    <generator>Hugo</generator>
    <language>zh-cn</language>
    <lastBuildDate>Thu, 17 Apr 2025 00:03:29 +0800</lastBuildDate>
    <atom:link href="https://example.org/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Variational Autoencoder</title>
      <link>https://example.org/posts/variational-autoencoder/</link>
      <pubDate>Thu, 17 Apr 2025 00:03:29 +0800</pubDate>
      <guid>https://example.org/posts/variational-autoencoder/</guid>
      <description>&lt;h2 id=&#34;生成模型的目标&#34;&gt;生成模型的目标&lt;/h2&gt;&#xA;&lt;p&gt;生成模型（Generative Models）的目的是想学习真实数据分布 $p(x)$， 其中 $X$ 通常是定义在某个（高维）空间 $\mathcal{X}$ 上的数据点。比如一张图像就是一个高维数据点，每个像素对应一个维度。具体来讲生成模型想要解决的问题：&lt;em&gt;&lt;strong&gt;考虑一个从真实分布 $p(x)$ 中采样得到的数据集 $ \lbrace{x_1, x_2, \dots, x_n \rbrace}$  ，我们希望从采样得到的数据子集中学习一个分布 $p_\theta(x)$ ，逼近真实分布 $p(x)$&lt;/strong&gt;&lt;/em&gt;。&lt;/p&gt;&#xA;&lt;h2 id=&#34;变分自编码器-variational-autoencoder&#34;&gt;变分自编码器 Variational Autoencoder&lt;/h2&gt;&#xA;&lt;p&gt;变分自编码器（VAE）作为一种生成模型，依然在现在的机器学习算法占有一席之地。VAE的优化目标推导其实有好几种方式，在开始之前，我想先从最简单的例子开始。&lt;/p&gt;&#xA;&lt;h3 id=&#34;简单假设下存在的问题&#34;&gt;简单假设下存在的问题&lt;/h3&gt;&#xA;&lt;p&gt;考虑对人脸数据集CelebA的建模，我们希望从CelebA数据集中学习到分布 $p_\theta(x)$，然后从 $p_\theta(x)$ 中采样得到新的人脸样本。从流形假设（Manifold Hypothesis）的角度来讲，自然图像数据在高维像素空间中形成一个稠密子集，其内在结构可以用一个低维、非线性流形来近似建模；或者说，图像数据服从一个 &lt;em&gt;&lt;strong&gt;嵌入在高维像素空间中低维非线性流形分布&lt;/strong&gt;&lt;/em&gt; 。以CelebA为例，每张图像的数据维度为178x218x3维，RGB图像每一维有256种取值，这个一个非常庞大的高维空间，只有极少数组合才对应一张“真实的人脸”，实际上影响人脸的因素可以抽象为具体几类（比如表情，年龄，肤色，五官轮廓等等）。当然，具体抽象成哪些类别并不是我们关心的问题，我们关心的是高维（图像）数据 $x$ 到低维空间隐变量 $z$（latent variables）的映射关系，通过构建这对映射关系，我们能够实现从 $p(z)$ 中采样，生成新样本 $ \hat{x}$。其实深度学习中不少领域都与该流形假设有关，比如自编码器、表示学习、对抗样本等。&lt;/p&gt;&#xA;&lt;p&gt;基于上面的想法，一个很自然的想法浮现在脑海中：可以直接构建一个解码器（Decoder），从先验分布 $p(z)$ 中采样，作为Decoder的输入，生成样本并和真实分布中的数据求距离：&#xA;&lt;/p&gt;&#xA;$$&#xA;\begin{equation}&#xA;\begin{aligned}&#xA;p_{\theta}(X) &amp;= \int p_{\theta}(X|z)p(z) dz \\&#xA;     &amp;= \int \mathcal{N}(X|f(z;\theta), \Sigma) \cdot \mathcal{N}(z|0, I) dz \\&#xA;     &amp;= \mathbb{E}_{z \sim p(z)} \left[ p_{\theta}(X|z) \right] \\&#xA;     &amp;\approx \frac{1}{m} \sum_{i=0}^{m} p_{\theta}(X|z_{i})&#xA;\end{aligned}&#xA;\end{equation}&#xA;$$&lt;p&gt;&#xA;其中，$f(z;\theta)$ 是隐变量 $z$ 到样本空间 $ X$ 的映射函数，在这里也就是Decoder，隐变量 $z$ 通常假设为服从均值为 $0$，协方差矩阵为单元矩阵 $I$ 的高斯分布 $\mathcal{N}(z|0, I) $；Decoder生成的样本分布 $p_{\theta}(X|z)$ 的均值，协方差矩阵 $\Sigma$ 一般设为常数。容易发现，我们利用蒙特卡洛采样（Monte Carlo Sampling）从 $p(z)$ 中采样，经过Decoder就可以生成新的样本了，然后计算损失，反向传播优化Decoder了。&#xA;&lt;figure&gt;&lt;img src=&#34;https://example.org/pic_vae/dec.png&#34; width=&#34;1600&#34;&gt;&#xA;&lt;/figure&gt;&#xA;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
