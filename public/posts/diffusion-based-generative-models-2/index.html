<!DOCTYPE html>
<html lang="zh-cn">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Diffusion-Based Generative Models &lt;2&gt;: DDIM | Renjie&#39;s log</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="一. 引言
上一篇内容讲了DDPM的算法框架，看起来一切都很完美，但采样速度还是太慢了，如果设置 $ T=1000$, 那采样的代价还是太大了。因此迎来了DDIM (Denoising Diffusion Implicit Models)。对于DDIM，我觉得还是可以从 DDPM和 SDE/ODE 两个角度去分析的。
1.1 DDPM视角下的DDIM
核心思想

DDPM 是一个基于马尔可夫链的扩散模型，通过逐步加噪（前向过程）和逐步去噪（反向过程）学习数据分布。
DDIM 是 DDPM 的 非马尔可夫推广，它重新参数化了反向过程，允许 跳过中间步骤，从而加速采样。

非马尔可夫性

DDPM：前向和反向过程都是马尔可夫的（下一步仅依赖当前步）。
DDIM：通过设计非马尔可夫的逆过程，打破了这一限制，允许更灵活的生成路径（如跳步采样）。

确定性生成

DDPM：反向过程是随机的（每一步注入高斯噪声）。
DDIM：可以通过设定噪声方差为0，实现 确定性生成（类似ODE），从而生成结果可重复。

采样加速

DDIM 通过重新参数化，将 DDPM 的 $T$ 步采样压缩到 $S$ 步（$S \ll T$），而保持相似的生成质量。

数学形式
DDIM 的逆过程改写为：

$$
x_{t-1} = \sqrt{\alpha_{t-1}} \left( \frac{x_t - \sqrt{1-\alpha_t} \epsilon_\theta(x_t, t)}{\sqrt{\alpha_t}} \right) &#43; \sqrt{1-\alpha_{t-1}} \epsilon_\theta(x_t, t)
$$
其中 $\alpha_t$ 是噪声调度，$\epsilon_\theta$ 是去噪网络。当噪声项系数为0时，生成过程变为确定性。

1.2. SDE/ODE视角下的DDIM
核心思想
扩散模型可以统一描述为 随机微分方程（SDE） 或 常微分方程（ODE） 的离散化：">
    <meta name="generator" content="Hugo 0.146.5">
    
    
    
      <meta name="robots" content="noindex, nofollow">
    
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



  


    


    
      

    

    

    
      <link rel="canonical" href="http://localhost:1313/posts/diffusion-based-generative-models-2/">
    

    <meta property="og:url" content="http://localhost:1313/posts/diffusion-based-generative-models-2/">
  <meta property="og:site_name" content="Renjie&#39;s log">
  <meta property="og:title" content="Diffusion-Based Generative Models &lt;2&gt;: DDIM">
  <meta property="og:description" content="一. 引言 上一篇内容讲了DDPM的算法框架，看起来一切都很完美，但采样速度还是太慢了，如果设置 $ T=1000$, 那采样的代价还是太大了。因此迎来了DDIM (Denoising Diffusion Implicit Models)。对于DDIM，我觉得还是可以从 DDPM和 SDE/ODE 两个角度去分析的。
1.1 DDPM视角下的DDIM 核心思想 DDPM 是一个基于马尔可夫链的扩散模型，通过逐步加噪（前向过程）和逐步去噪（反向过程）学习数据分布。 DDIM 是 DDPM 的 非马尔可夫推广，它重新参数化了反向过程，允许 跳过中间步骤，从而加速采样。 非马尔可夫性 DDPM：前向和反向过程都是马尔可夫的（下一步仅依赖当前步）。 DDIM：通过设计非马尔可夫的逆过程，打破了这一限制，允许更灵活的生成路径（如跳步采样）。 确定性生成 DDPM：反向过程是随机的（每一步注入高斯噪声）。 DDIM：可以通过设定噪声方差为0，实现 确定性生成（类似ODE），从而生成结果可重复。 采样加速 DDIM 通过重新参数化，将 DDPM 的 $T$ 步采样压缩到 $S$ 步（$S \ll T$），而保持相似的生成质量。 数学形式 DDIM 的逆过程改写为： $$ x_{t-1} = \sqrt{\alpha_{t-1}} \left( \frac{x_t - \sqrt{1-\alpha_t} \epsilon_\theta(x_t, t)}{\sqrt{\alpha_t}} \right) &#43; \sqrt{1-\alpha_{t-1}} \epsilon_\theta(x_t, t) $$ 其中 $\alpha_t$ 是噪声调度，$\epsilon_\theta$ 是去噪网络。当噪声项系数为0时，生成过程变为确定性。
1.2. SDE/ODE视角下的DDIM 核心思想 扩散模型可以统一描述为 随机微分方程（SDE） 或 常微分方程（ODE） 的离散化：">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-06-24T20:08:36+08:00">
    <meta property="article:modified_time" content="2025-06-24T20:08:36+08:00">
    <meta property="article:tag" content="Diffusion-Models">
    <meta property="article:tag" content="Deep-Learning">
    <meta property="article:tag" content="Generative-AI">

  <meta itemprop="name" content="Diffusion-Based Generative Models &lt;2&gt;: DDIM">
  <meta itemprop="description" content="一. 引言 上一篇内容讲了DDPM的算法框架，看起来一切都很完美，但采样速度还是太慢了，如果设置 $ T=1000$, 那采样的代价还是太大了。因此迎来了DDIM (Denoising Diffusion Implicit Models)。对于DDIM，我觉得还是可以从 DDPM和 SDE/ODE 两个角度去分析的。
1.1 DDPM视角下的DDIM 核心思想 DDPM 是一个基于马尔可夫链的扩散模型，通过逐步加噪（前向过程）和逐步去噪（反向过程）学习数据分布。 DDIM 是 DDPM 的 非马尔可夫推广，它重新参数化了反向过程，允许 跳过中间步骤，从而加速采样。 非马尔可夫性 DDPM：前向和反向过程都是马尔可夫的（下一步仅依赖当前步）。 DDIM：通过设计非马尔可夫的逆过程，打破了这一限制，允许更灵活的生成路径（如跳步采样）。 确定性生成 DDPM：反向过程是随机的（每一步注入高斯噪声）。 DDIM：可以通过设定噪声方差为0，实现 确定性生成（类似ODE），从而生成结果可重复。 采样加速 DDIM 通过重新参数化，将 DDPM 的 $T$ 步采样压缩到 $S$ 步（$S \ll T$），而保持相似的生成质量。 数学形式 DDIM 的逆过程改写为： $$ x_{t-1} = \sqrt{\alpha_{t-1}} \left( \frac{x_t - \sqrt{1-\alpha_t} \epsilon_\theta(x_t, t)}{\sqrt{\alpha_t}} \right) &#43; \sqrt{1-\alpha_{t-1}} \epsilon_\theta(x_t, t) $$ 其中 $\alpha_t$ 是噪声调度，$\epsilon_\theta$ 是去噪网络。当噪声项系数为0时，生成过程变为确定性。
1.2. SDE/ODE视角下的DDIM 核心思想 扩散模型可以统一描述为 随机微分方程（SDE） 或 常微分方程（ODE） 的离散化：">
  <meta itemprop="datePublished" content="2025-06-24T20:08:36+08:00">
  <meta itemprop="dateModified" content="2025-06-24T20:08:36+08:00">
  <meta itemprop="wordCount" content="182">
  <meta itemprop="keywords" content="Diffusion-Models,Deep-Learning,Generative-AI">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Diffusion-Based Generative Models &lt;2&gt;: DDIM">
  <meta name="twitter:description" content="一. 引言 上一篇内容讲了DDPM的算法框架，看起来一切都很完美，但采样速度还是太慢了，如果设置 $ T=1000$, 那采样的代价还是太大了。因此迎来了DDIM (Denoising Diffusion Implicit Models)。对于DDIM，我觉得还是可以从 DDPM和 SDE/ODE 两个角度去分析的。
1.1 DDPM视角下的DDIM 核心思想 DDPM 是一个基于马尔可夫链的扩散模型，通过逐步加噪（前向过程）和逐步去噪（反向过程）学习数据分布。 DDIM 是 DDPM 的 非马尔可夫推广，它重新参数化了反向过程，允许 跳过中间步骤，从而加速采样。 非马尔可夫性 DDPM：前向和反向过程都是马尔可夫的（下一步仅依赖当前步）。 DDIM：通过设计非马尔可夫的逆过程，打破了这一限制，允许更灵活的生成路径（如跳步采样）。 确定性生成 DDPM：反向过程是随机的（每一步注入高斯噪声）。 DDIM：可以通过设定噪声方差为0，实现 确定性生成（类似ODE），从而生成结果可重复。 采样加速 DDIM 通过重新参数化，将 DDPM 的 $T$ 步采样压缩到 $S$ 步（$S \ll T$），而保持相似的生成质量。 数学形式 DDIM 的逆过程改写为： $$ x_{t-1} = \sqrt{\alpha_{t-1}} \left( \frac{x_t - \sqrt{1-\alpha_t} \epsilon_\theta(x_t, t)}{\sqrt{\alpha_t}} \right) &#43; \sqrt{1-\alpha_{t-1}} \epsilon_\theta(x_t, t) $$ 其中 $\alpha_t$ 是噪声调度，$\epsilon_\theta$ 是去噪网络。当噪声项系数为0时，生成过程变为确定性。
1.2. SDE/ODE视角下的DDIM 核心思想 扩散模型可以统一描述为 随机微分方程（SDE） 或 常微分方程（ODE） 的离散化：">

	




<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  window.MathJax  = {
    tex: {
        inlineMath: [['$', '$'], ['\$', '\$']], 
        displayMath: [['$$', '$$']],
        processEnvironments: true,
        
        packages: ['base', 'ams', 'noerrors', 'noundefined'],
        tags: "ams",
    },
    loader:{
        load: ['ui/safe', '[tex]/ams'], 
    
    },
  };
</script>

<link rel="stylesheet" href="/css/custom.css">


  </head><body class="ma0 avenir bg-near-white development">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l center items-center justify-between">
    <a href="/" class="f3 fw2 hover-white white-90 dib no-underline">
      
        Renjie&#39;s log
      
    </a>
    <div class="flex-l items-center">
      

      
      <div class="ananke-socials"></div>

    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  
  <article class="flex-l mw8 center ph3 flex-wrap justify-between">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked ttu">
          
        Posts
      </aside><div id="sharing" class="mt3 ananke-socials"></div>
<h1 class="f1 athelas mt3 mb1">Diffusion-Based Generative Models &lt;2&gt;: DDIM</h1>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2025-06-24T20:08:36+08:00">June 24, 2025</time>
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><h2 id="一-引言">一. 引言</h2>
<p>上一篇内容讲了DDPM的算法框架，看起来一切都很完美，但采样速度还是太慢了，如果设置 $ T=1000$, 那采样的代价还是太大了。因此迎来了DDIM (Denoising Diffusion Implicit Models)。对于DDIM，我觉得还是可以从 DDPM和 SDE/ODE 两个角度去分析的。</p>
<h3 id="11-ddpm视角下的ddim">1.1 DDPM视角下的DDIM</h3>
<h4 id="核心思想">核心思想</h4>
<ul>
<li><strong>DDPM</strong> 是一个基于马尔可夫链的扩散模型，通过逐步加噪（前向过程）和逐步去噪（反向过程）学习数据分布。</li>
<li><strong>DDIM</strong> 是 DDPM 的 <strong>非马尔可夫推广</strong>，它重新参数化了反向过程，允许 <strong>跳过中间步骤</strong>，从而加速采样。</li>
</ul>
<h4 id="非马尔可夫性">非马尔可夫性</h4>
<ul>
<li><strong>DDPM</strong>：前向和反向过程都是马尔可夫的（下一步仅依赖当前步）。</li>
<li><strong>DDIM</strong>：通过设计非马尔可夫的逆过程，打破了这一限制，允许更灵活的生成路径（如跳步采样）。</li>
</ul>
<h4 id="确定性生成">确定性生成</h4>
<ul>
<li><strong>DDPM</strong>：反向过程是随机的（每一步注入高斯噪声）。</li>
<li><strong>DDIM</strong>：可以通过设定噪声方差为0，实现 <strong>确定性生成</strong>（类似ODE），从而生成结果可重复。</li>
</ul>
<h4 id="采样加速">采样加速</h4>
<ul>
<li>DDIM 通过重新参数化，将 DDPM 的 $T$ 步采样压缩到 $S$ 步（$S \ll T$），而保持相似的生成质量。</li>
</ul>
<h4 id="数学形式">数学形式</h4>
<p>DDIM 的逆过程改写为：
</p>
$$
x_{t-1} = \sqrt{\alpha_{t-1}} \left( \frac{x_t - \sqrt{1-\alpha_t} \epsilon_\theta(x_t, t)}{\sqrt{\alpha_t}} \right) + \sqrt{1-\alpha_{t-1}} \epsilon_\theta(x_t, t)
$$<p>
其中 $\alpha_t$ 是噪声调度，$\epsilon_\theta$ 是去噪网络。当噪声项系数为0时，生成过程变为确定性。</p>
<hr>
<h3 id="12-sdeode视角下的ddim">1.2. SDE/ODE视角下的DDIM</h3>
<h4 id="核心思想-1">核心思想</h4>
<p>扩散模型可以统一描述为 <strong>随机微分方程（SDE）</strong> 或 <strong>常微分方程（ODE）</strong> 的离散化：</p>
<ul>
<li><strong>SDE</strong>：前向过程是加噪的随机过程，反向过程对应一个逆时间的SDE。</li>
<li><strong>ODE</strong>：通过Fokker-Planck方程可证明，任何逆向SDE均存在一个确定性ODE，其解与SDE共享相同的边缘概率分布 $ p_t(x) $。 忽略SDE中的随机噪声项，即可导出确定性生成路径（概率流ODE），适合快速采样和确定性生成。</li>
</ul>
<p>概率流ODE的连续形式：</p>
$$
dx = \left[ f(x, t) - \frac{1}{2}g(t)^2 \nabla_x \log p_t(x) \right] dt
$$<p><strong>DDIM</strong> 是 <strong>ODE离散化的一种特例</strong>，其生成路径对应概率流ODE的数值解法。DDPM和DDIM原文中的采样方法，本质上分别对应 SDE和 ODE的一阶离散化，比如欧拉-丸山法和欧拉法。本篇就不展开讲SDE/ODE视角下的DDIM了，等后面站在 SDE/ODE大一统的视角下去看，一切就都明朗了。</p>
<h2 id="二-ddim算法框架">二. DDIM算法框架</h2>
<p>DDIM算法框架分成两部分讲：</p>
<ul>
<li>DDPM的非马尔可夫推广</li>
<li>加速采样</li>
</ul>
<h3 id="ddpm的非马尔可夫推广">DDPM的非马尔可夫推广</h3>
<!-- 插句题外话 -->
<p>回顾DDPM的优化目标 $\mathcal{L}_{VLB}$ :</p>
$$
\mathcal{L}_{VLB} = - \sum_{t=1}^T \frac{1}{2\sigma_{q}^2(t)} \cdot \frac{(1 - \alpha_t)^2 }{\alpha_t(1 - \bar{\alpha}_t)} \mathbb{E}_{q(\mathbf{x_{t}}|\mathbf{x_0})} \left[ \| \epsilon_t - \epsilon_\theta(\mathbf{x_t}, t)\|^2 \right]
$$<p>DDIM的核心动机，就来源于DDPM的目标函数只依赖于边际分布 $q(\mathbf{x_t}|\mathbf{x_0})$，
而不是联合分布 $q(\mathbf{x_{1:T}}|\mathbf{x_0})$。</p>
<p>再回顾DDPM优化目标推导过程的一个中间表达形式：</p>
$$
\frac{1}{2\sigma_{q}^2(t)} \|  \mu_q(\mathbf{x}_t, \mathbf{x}_0) -  \mu_{\theta}(\mathbf{x}_t, t) \|^2
$$<p>仔细想想不难发现，其实我们关心的只有 $q(\mathbf{x_t}|\mathbf{x_0})$ 和 $q(\mathbf{x_{t-1}}|\mathbf{x_0}, \mathbf{x_t})$ 的均值 $\mu_q(\mathbf{x}_t, \mathbf{x}_0)$</p>
<p>通过对联合分布进行合适的分解，我们可以在保持DDPM优化目标不变的前提下，完成非马尔可夫链的推广形式：</p>
$$
\begin{aligned}
q(\mathbf{x}_{t-1} | \mathbf{x}_t, \mathbf{x}_0) &= \frac{q(\mathbf{x}_t | \mathbf{x}_{t-1}, \mathbf{x}_0) \cdot q(\mathbf{x}_{t-1} | \mathbf{x}_0)}{q(\mathbf{x_{t}} | \mathbf{x}_0)} \\
&= \frac{q(\mathbf{x}_t | \mathbf{x}_{t-1}) \cdot q(\mathbf{x}_{t-1} | \mathbf{x}_0)}{q(\mathbf{x_{t}} | \mathbf{x}_0)}
\end{aligned}
$$<figure><img src="/pic_diff_2/non-markov-forward.png">
</figure>

<ul class="pa0">
  
   <li class="list di">
     <a href="/tags/diffusion-models/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">Diffusion-Models</a>
   </li>
  
   <li class="list di">
     <a href="/tags/deep-learning/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">Deep-Learning</a>
   </li>
  
   <li class="list di">
     <a href="/tags/generative-ai/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">Generative-AI</a>
   </li>
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




  <div class="bg-light-gray pa3 nested-list-reset nested-copy-line-height nested-links">
    <p class="f5 b mb3">Related</p>
    <ul class="pa0 list">
	   
	     <li  class="mb2">
          <a href="/posts/diffusion-based-generative-models-1/">Diffusion-Based Generative Models &lt;1&gt;: DDPM</a>
        </li>
	    
    </ul>
</div>

</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white white-70 dn dib-ns pv2 ph3 no-underline" href="http://localhost:1313/" >
    &copy;  Renjie's log 2025 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

  </body>
</html>
