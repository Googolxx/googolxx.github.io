<!DOCTYPE html>
<html lang="zh-cn">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Diffusion-Based Generative Models &lt;5&gt;: SDE/ODE视角下的扩散模型 | Renjie&#39;s log</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="一. SDE/ODE
在前面的章节中，我们详细讨论了扩散模型的概率论基础，包括维纳过程、Itô积分、随机微分方程和Fokker-Planck方程。现在我们将这些理论工具应用到扩散模型中，从SDE/ODE的视角来理解扩散过程。
扩散模型的核心思想是将数据分布通过前向过程逐渐转换为噪声分布，然后学习反向过程从噪声恢复数据。这个过程可以用随机微分方程（SDE）来描述，而对应的确定性过程可以用常微分方程（ODE）来描述。
1.1 基本形式

考虑一个向量形式的SDE：
$$
d\mathbf{x} = \mathbf{f}(\mathbf{x}, t)dt &#43; \mathbf{G}(t) d\mathbf{W}
$$其中 $\mathbf{x} \in \mathbb{R}^d$，$\mathbf{f}: \mathbb{R}^d \times [0, T] \to \mathbb{R}^d$ 是漂移函数，$\mathbf{G}: [0, T] \to \mathbb{R}^{d \times d}$ 是扩散矩阵，$\mathbf{W}$ 是 $d$ 维维纳过程。
1.2 逆向SDE
对于前向SDE：
$$
d\mathbf{x} = \mathbf{f}(\mathbf{x}, t)dt &#43; \mathbf{G}(t) d\mathbf{W}
$$其逆向SDE为（逆向SDE的推导基于Girsanov定理和Fokker-Planck方程）：
$$
d\mathbf{x} = \left[\mathbf{f}(\mathbf{x}, t) - \mathbf{G}(t)\mathbf{G}(t)^T \nabla_{\mathbf{x}} \log p_t(\mathbf{x})\right]dt &#43; \mathbf{G}(t) d\bar{\mathbf{W}}
$$其中 $\bar{\mathbf{W}}$ 是逆向时间维纳过程，$\nabla_{\mathbf{x}} \log p_t(\mathbf{x})$ 是时间 $t$ 时刻状态 $\mathbf{x}$ 的对数概率密度梯度。">
    <meta name="generator" content="Hugo 0.146.5">
    
    
    
      <meta name="robots" content="noindex, nofollow">
    
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



  


    


    
      

    

    

    
      <link rel="canonical" href="http://localhost:1313/posts/diffusion-based-generative-models-5/">
    

    <meta property="og:url" content="http://localhost:1313/posts/diffusion-based-generative-models-5/">
  <meta property="og:site_name" content="Renjie&#39;s log">
  <meta property="og:title" content="Diffusion-Based Generative Models &lt;5&gt;: SDE/ODE视角下的扩散模型">
  <meta property="og:description" content="一. SDE/ODE 在前面的章节中，我们详细讨论了扩散模型的概率论基础，包括维纳过程、Itô积分、随机微分方程和Fokker-Planck方程。现在我们将这些理论工具应用到扩散模型中，从SDE/ODE的视角来理解扩散过程。
扩散模型的核心思想是将数据分布通过前向过程逐渐转换为噪声分布，然后学习反向过程从噪声恢复数据。这个过程可以用随机微分方程（SDE）来描述，而对应的确定性过程可以用常微分方程（ODE）来描述。
1.1 基本形式 考虑一个向量形式的SDE：
$$ d\mathbf{x} = \mathbf{f}(\mathbf{x}, t)dt &#43; \mathbf{G}(t) d\mathbf{W} $$其中 $\mathbf{x} \in \mathbb{R}^d$，$\mathbf{f}: \mathbb{R}^d \times [0, T] \to \mathbb{R}^d$ 是漂移函数，$\mathbf{G}: [0, T] \to \mathbb{R}^{d \times d}$ 是扩散矩阵，$\mathbf{W}$ 是 $d$ 维维纳过程。
1.2 逆向SDE 对于前向SDE：
$$ d\mathbf{x} = \mathbf{f}(\mathbf{x}, t)dt &#43; \mathbf{G}(t) d\mathbf{W} $$其逆向SDE为（逆向SDE的推导基于Girsanov定理和Fokker-Planck方程）：
$$ d\mathbf{x} = \left[\mathbf{f}(\mathbf{x}, t) - \mathbf{G}(t)\mathbf{G}(t)^T \nabla_{\mathbf{x}} \log p_t(\mathbf{x})\right]dt &#43; \mathbf{G}(t) d\bar{\mathbf{W}} $$其中 $\bar{\mathbf{W}}$ 是逆向时间维纳过程，$\nabla_{\mathbf{x}} \log p_t(\mathbf{x})$ 是时间 $t$ 时刻状态 $\mathbf{x}$ 的对数概率密度梯度。">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-07-15T21:20:39+08:00">
    <meta property="article:modified_time" content="2025-07-15T21:20:39+08:00">
    <meta property="article:tag" content="Diffusion-Models">
    <meta property="article:tag" content="Deep-Learning">
    <meta property="article:tag" content="Generative-AI">

  <meta itemprop="name" content="Diffusion-Based Generative Models &lt;5&gt;: SDE/ODE视角下的扩散模型">
  <meta itemprop="description" content="一. SDE/ODE 在前面的章节中，我们详细讨论了扩散模型的概率论基础，包括维纳过程、Itô积分、随机微分方程和Fokker-Planck方程。现在我们将这些理论工具应用到扩散模型中，从SDE/ODE的视角来理解扩散过程。
扩散模型的核心思想是将数据分布通过前向过程逐渐转换为噪声分布，然后学习反向过程从噪声恢复数据。这个过程可以用随机微分方程（SDE）来描述，而对应的确定性过程可以用常微分方程（ODE）来描述。
1.1 基本形式 考虑一个向量形式的SDE：
$$ d\mathbf{x} = \mathbf{f}(\mathbf{x}, t)dt &#43; \mathbf{G}(t) d\mathbf{W} $$其中 $\mathbf{x} \in \mathbb{R}^d$，$\mathbf{f}: \mathbb{R}^d \times [0, T] \to \mathbb{R}^d$ 是漂移函数，$\mathbf{G}: [0, T] \to \mathbb{R}^{d \times d}$ 是扩散矩阵，$\mathbf{W}$ 是 $d$ 维维纳过程。
1.2 逆向SDE 对于前向SDE：
$$ d\mathbf{x} = \mathbf{f}(\mathbf{x}, t)dt &#43; \mathbf{G}(t) d\mathbf{W} $$其逆向SDE为（逆向SDE的推导基于Girsanov定理和Fokker-Planck方程）：
$$ d\mathbf{x} = \left[\mathbf{f}(\mathbf{x}, t) - \mathbf{G}(t)\mathbf{G}(t)^T \nabla_{\mathbf{x}} \log p_t(\mathbf{x})\right]dt &#43; \mathbf{G}(t) d\bar{\mathbf{W}} $$其中 $\bar{\mathbf{W}}$ 是逆向时间维纳过程，$\nabla_{\mathbf{x}} \log p_t(\mathbf{x})$ 是时间 $t$ 时刻状态 $\mathbf{x}$ 的对数概率密度梯度。">
  <meta itemprop="datePublished" content="2025-07-15T21:20:39+08:00">
  <meta itemprop="dateModified" content="2025-07-15T21:20:39+08:00">
  <meta itemprop="wordCount" content="2313">
  <meta itemprop="keywords" content="Diffusion-Models,Deep-Learning,Generative-AI">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Diffusion-Based Generative Models &lt;5&gt;: SDE/ODE视角下的扩散模型">
  <meta name="twitter:description" content="一. SDE/ODE 在前面的章节中，我们详细讨论了扩散模型的概率论基础，包括维纳过程、Itô积分、随机微分方程和Fokker-Planck方程。现在我们将这些理论工具应用到扩散模型中，从SDE/ODE的视角来理解扩散过程。
扩散模型的核心思想是将数据分布通过前向过程逐渐转换为噪声分布，然后学习反向过程从噪声恢复数据。这个过程可以用随机微分方程（SDE）来描述，而对应的确定性过程可以用常微分方程（ODE）来描述。
1.1 基本形式 考虑一个向量形式的SDE：
$$ d\mathbf{x} = \mathbf{f}(\mathbf{x}, t)dt &#43; \mathbf{G}(t) d\mathbf{W} $$其中 $\mathbf{x} \in \mathbb{R}^d$，$\mathbf{f}: \mathbb{R}^d \times [0, T] \to \mathbb{R}^d$ 是漂移函数，$\mathbf{G}: [0, T] \to \mathbb{R}^{d \times d}$ 是扩散矩阵，$\mathbf{W}$ 是 $d$ 维维纳过程。
1.2 逆向SDE 对于前向SDE：
$$ d\mathbf{x} = \mathbf{f}(\mathbf{x}, t)dt &#43; \mathbf{G}(t) d\mathbf{W} $$其逆向SDE为（逆向SDE的推导基于Girsanov定理和Fokker-Planck方程）：
$$ d\mathbf{x} = \left[\mathbf{f}(\mathbf{x}, t) - \mathbf{G}(t)\mathbf{G}(t)^T \nabla_{\mathbf{x}} \log p_t(\mathbf{x})\right]dt &#43; \mathbf{G}(t) d\bar{\mathbf{W}} $$其中 $\bar{\mathbf{W}}$ 是逆向时间维纳过程，$\nabla_{\mathbf{x}} \log p_t(\mathbf{x})$ 是时间 $t$ 时刻状态 $\mathbf{x}$ 的对数概率密度梯度。">

	




<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  window.MathJax  = {
    tex: {
        inlineMath: [['$', '$'], ['\$', '\$']], 
        displayMath: [['$$', '$$']],
        processEnvironments: true,
        
        packages: ['base', 'ams', 'noerrors', 'noundefined'],
        tags: "ams",
    },
    loader:{
        load: ['ui/safe', '[tex]/ams'], 
    
    },
  };
</script>

<link rel="stylesheet" href="/css/custom.css">


  </head><body class="ma0 avenir bg-near-white development">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l center items-center justify-between">
    <a href="/" class="f3 fw2 hover-white white-90 dib no-underline">
      
        Renjie&#39;s log
      
    </a>
    <div class="flex-l items-center">
      

      
      <div class="ananke-socials"></div>

    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  
  <article class="flex-l mw8 center ph3 flex-wrap justify-between">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked ttu">
          
        Posts
      </aside><div id="sharing" class="mt3 ananke-socials"></div>
<h1 class="f1 athelas mt3 mb1">Diffusion-Based Generative Models &lt;5&gt;: SDE/ODE视角下的扩散模型</h1>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2025-07-15T21:20:39+08:00">July 15, 2025</time>
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><h2 id="一-sdeode">一. SDE/ODE</h2>
<p>在前面的章节中，我们详细讨论了扩散模型的概率论基础，包括维纳过程、Itô积分、随机微分方程和Fokker-Planck方程。现在我们将这些理论工具应用到扩散模型中，从SDE/ODE的视角来理解扩散过程。</p>
<p>扩散模型的核心思想是将数据分布通过前向过程逐渐转换为噪声分布，然后学习反向过程从噪声恢复数据。这个过程可以用随机微分方程（SDE）来描述，而对应的确定性过程可以用常微分方程（ODE）来描述。</p>
<h3 id="11-基本形式">1.1 基本形式</h3>
<!-- 考虑向量形式的ODE：

$$
d\mathbf{x} = \mathbf{f}(\mathbf{x}, t)dt
$$ -->
<p>考虑一个向量形式的SDE：</p>
$$
d\mathbf{x} = \mathbf{f}(\mathbf{x}, t)dt + \mathbf{G}(t) d\mathbf{W}
$$<p>其中 $\mathbf{x} \in \mathbb{R}^d$，$\mathbf{f}: \mathbb{R}^d \times [0, T] \to \mathbb{R}^d$ 是漂移函数，$\mathbf{G}: [0, T] \to \mathbb{R}^{d \times d}$ 是扩散矩阵，$\mathbf{W}$ 是 $d$ 维维纳过程。</p>
<h3 id="12-逆向sde">1.2 逆向SDE</h3>
<p>对于前向SDE：</p>
$$
d\mathbf{x} = \mathbf{f}(\mathbf{x}, t)dt + \mathbf{G}(t) d\mathbf{W}
$$<p>其逆向SDE为（逆向SDE的推导基于Girsanov定理和Fokker-Planck方程）：</p>
$$
d\mathbf{x} = \left[\mathbf{f}(\mathbf{x}, t) - \mathbf{G}(t)\mathbf{G}(t)^T \nabla_{\mathbf{x}} \log p_t(\mathbf{x})\right]dt + \mathbf{G}(t) d\bar{\mathbf{W}}
$$<p>其中 $\bar{\mathbf{W}}$ 是逆向时间维纳过程，$\nabla_{\mathbf{x}} \log p_t(\mathbf{x})$ 是时间 $t$ 时刻状态 $\mathbf{x}$ 的对数概率密度梯度。</p>
<!-- 
#### 1.2.1 逆向SDE的推导

逆向SDE的推导基于以下关键思想：

1. **Girsanov定理**：通过改变测度实现时间反向
2. **Fokker-Planck方程**：描述概率密度的演化
3. **分数函数**：对数概率密度的梯度

**步骤1：Fokker-Planck方程**

前向SDE对应的Fokker-Planck方程为：

$$\frac{\partial p_t(\mathbf{x})}{\partial t} = -\nabla \cdot [\mathbf{f}(\mathbf{x}, t)p_t(\mathbf{x})] + \frac{1}{2}\nabla \cdot [\mathbf{G}(t)\mathbf{G}(t)^T \nabla p_t(\mathbf{x})]$$

**步骤2：概率流分解**

将概率流分解为漂移项和扩散项：

$$\frac{\partial p_t(\mathbf{x})}{\partial t} = -\nabla \cdot [\mathbf{v}(\mathbf{x}, t)p_t(\mathbf{x})]$$

其中 $\mathbf{v}(\mathbf{x}, t)$ 是概率流速度：

$$\mathbf{v}(\mathbf{x}, t) = \mathbf{f}(\mathbf{x}, t) - \frac{1}{2}\mathbf{G}(t)\mathbf{G}(t)^T \nabla_{\mathbf{x}} \log p_t(\mathbf{x})$$

**步骤3：逆向过程**

逆向过程需要满足：

$$\frac{\partial p_t(\mathbf{x})}{\partial t} = \nabla \cdot [\mathbf{v}(\mathbf{x}, t)p_t(\mathbf{x})]$$

这对应于逆向SDE：

$$d\mathbf{x} = \mathbf{v}(\mathbf{x}, t)dt + \mathbf{G}(t)d\bar{\mathbf{W}}$$

#### 1.2.2 分数函数的作用

分数函数 $\nabla_{\mathbf{x}} \log p_t(\mathbf{x})$ 在逆向过程中起到关键作用：

1. **方向指导**：指向概率密度增加的方向
2. **强度控制**：控制去噪的强度
3. **局部信息**：只依赖当前点的局部概率结构

对于高斯分布 $\mathcal{N}(\mathbf{x}; \boldsymbol{\mu}, \boldsymbol{\Sigma})$：

$$\nabla_{\mathbf{x}} \log p(\mathbf{x}) = -\boldsymbol{\Sigma}^{-1}(\mathbf{x} - \boldsymbol{\mu})$$

这表明分数函数指向分布的中心，强度与到中心的距离成正比。 -->
<h3 id="13-简单sde分析示例">1.3 简单SDE分析示例</h3>
<p>为了更好地理解SDE和逆向SDE，我们分析一个简单的向量例子：$d\mathbf{x} = \mathbf{A} d\mathbf{W}$，其中 $\mathbf{A}$ 是常数矩阵。</p>
<h4 id="131-前向过程分析">1.3.1 前向过程分析</h4>
<p>对于SDE $d\mathbf{x} = \mathbf{A} d\mathbf{W}$：</p>
<ol>
<li><strong>漂移函数</strong>：$\mathbf{f}(\mathbf{x}, t) = \mathbf{0}$（无漂移）</li>
<li><strong>扩散矩阵</strong>：$\mathbf{G}(t) = \mathbf{A}$（常数扩散矩阵）</li>
<li><strong>初始条件</strong>：假设 $\mathbf{x}(0) = \mathbf{0}$</li>
</ol>
<h4 id="132-概率密度演化">1.3.2 概率密度演化</h4>
<p>对应的Fokker-Planck方程为：</p>
$$\frac{\partial p_t(\mathbf{x})}{\partial t} = \frac{1}{2} \nabla \cdot [\mathbf{A}\mathbf{A}^T \nabla p_t(\mathbf{x})]$$<p>这是一个多维热传导方程，其解为：</p>
$$p_t(\mathbf{x}) = \frac{1}{\sqrt{(2\pi)^d |\mathbf{A}\mathbf{A}^T| t}} \exp\left(-\frac{1}{2t} \mathbf{x}^T (\mathbf{A}\mathbf{A}^T)^{-1} \mathbf{x}\right)$$<p>即 $\mathbf{x}(t) \sim \mathcal{N}(\mathbf{0}, \mathbf{A}\mathbf{A}^T t)$，协方差矩阵随时间线性增长。</p>
<h4 id="133-逆向过程">1.3.3 逆向过程</h4>
<p>根据逆向SDE公式：</p>
$$d\mathbf{x} = \left[\mathbf{0} - \mathbf{A}\mathbf{A}^T \nabla_{\mathbf{x}} \log p_t(\mathbf{x})\right]dt + \mathbf{A} d\bar{\mathbf{W}}$$<p>计算分数函数：</p>
$$\nabla_{\mathbf{x}} \log p_t(\mathbf{x}) = \nabla_{\mathbf{x}} \left[-\frac{1}{2t} \mathbf{x}^T (\mathbf{A}\mathbf{A}^T)^{-1} \mathbf{x} - \frac{1}{2}\log((2\pi)^d |\mathbf{A}\mathbf{A}^T| t)\right] = -\frac{1}{t} (\mathbf{A}\mathbf{A}^T)^{-1} \mathbf{x}$$<p>因此逆向SDE为：</p>
$$d\mathbf{x} = \frac{1}{t} \mathbf{x} dt + \mathbf{A} d\bar{\mathbf{W}}$$<h4 id="134-物理解释">1.3.4 物理解释</h4>
<ul>
<li><strong>前向过程</strong>：从确定性初始状态 $\mathbf{x}(0) = \mathbf{0}$ 开始，通过多维随机游走扩散到多维高斯分布</li>
<li><strong>逆向过程</strong>：从多维高斯分布开始，通过漂移项 $\frac{1}{t} \mathbf{x}$ 将分布收缩回原点</li>
<li><strong>漂移项作用</strong>：$\frac{1}{t} \mathbf{x}$ 项提供了向原点的&quot;拉力&quot;，抵消了扩散效应</li>
</ul>
<p>这个向量例子展示了多维SDE如何描述概率分布的演化，以及逆向SDE如何实现分布的收缩。</p>
<h4 id="135-python模拟">1.3.5 Python模拟</h4>
<p>考虑一个简单的情况，扩散矩阵为单位矩阵的情况，模拟前向和逆向扩散过程的演化过程：</p>
<p><figure><img src="/pic_diff_5/sde_diffusion_visualization.png">
</figure>

<figure><img src="/pic_diff_5/sde_trajectories.png">
</figure>
</p>
<h2 id="二-sdeode数值求解">二. SDE/ODE数值求解</h2>
<h3 id="21-基本方法">2.1 基本方法</h3>
<h4 id="211-ode求解">2.1.1 ODE求解</h4>
<p>对于 $\frac{d\mathbf{x}}{dt} = \mathbf{f}(\mathbf{x}, t)$：</p>
<p><strong>欧拉方法</strong>：</p>
$$\mathbf{x}_{i+1} = \mathbf{x}_i + h \mathbf{f}(\mathbf{x}_i, t_i)$$<p><strong>收敛性分析</strong>：</p>
<ul>
<li>局部截断误差：$O(h^2)$</li>
<li>全局截断误差：$O(h)$</li>
<li>稳定性：条件稳定，步长需要足够小</li>
</ul>
<p><strong>RK4方法</strong>：
</p>
$$\begin{aligned}
\mathbf{k}_1 &= \mathbf{f}(\mathbf{x}_i, t_i) \\
\mathbf{k}_2 &= \mathbf{f}(\mathbf{x}_i + \frac{h}{2}\mathbf{k}_1, t_i + \frac{h}{2}) \\
\mathbf{k}_3 &= \mathbf{f}(\mathbf{x}_i + \frac{h}{2}\mathbf{k}_2, t_i + \frac{h}{2}) \\
\mathbf{k}_4 &= \mathbf{f}(\mathbf{x}_i + h\mathbf{k}_3, t_i + h) \\
\mathbf{x}_{i+1} &= \mathbf{x}_i + \frac{h}{6}(\mathbf{k}_1 + 2\mathbf{k}_2 + 2\mathbf{k}_3 + \mathbf{k}_4)
\end{aligned}$$<p><strong>收敛性分析</strong>：</p>
<ul>
<li>局部截断误差：$O(h^5)$</li>
<li>全局截断误差：$O(h^4)$</li>
<li>稳定性：更好的稳定性，适合刚性ODE</li>
</ul>
<p><strong>自适应步长方法</strong>：
</p>
$$h_{i+1} = h_i \left(\frac{\text{tol}}{\text{error}_i}\right)^{1/p}$$<p>其中 $\text{tol}$ 是容差，$p$ 是方法的阶数。</p>
<h4 id="212-sde求解">2.1.2 SDE求解</h4>
<p>对于 $d\mathbf{x} = \mathbf{f}(\mathbf{x}, t)dt + \mathbf{G}(t)d\mathbf{W}$：</p>
<p><strong>欧拉-丸山方法</strong>：
</p>
$$\mathbf{x}_{i+1} = \mathbf{x}_i + \mathbf{f}(\mathbf{x}_i, t_i)h + \mathbf{G}(t_i)\Delta\mathbf{W}_i$$<p>其中 $\Delta\mathbf{W}<em>i = \mathbf{W}(t</em>{i+1}) - \mathbf{W}(t_i) \sim \mathcal{N}(\mathbf{0}, h\mathbf{I})$。</p>
<p><strong>收敛性分析</strong>：</p>
<ul>
<li>强收敛阶：$O(h^{1/2})$（路径wise收敛）</li>
<li>弱收敛阶：$O(h)$（分布收敛）</li>
<li>优点：简单易实现</li>
<li>缺点：收敛阶较低</li>
</ul>
<p><strong>米尔斯坦方法</strong>：
</p>
$$\mathbf{x}_{i+1} = \mathbf{x}_i + \mathbf{f}(\mathbf{x}_i, t_i)h + \mathbf{G}(t_i)\Delta\mathbf{W}_i + \frac{1}{2}\mathbf{G}(t_i)\mathbf{G}(t_i)^T[(\Delta\mathbf{W}_i)^2 - h]$$<p><strong>收敛性分析</strong>：</p>
<ul>
<li>强收敛阶：$O(h)$</li>
<li>弱收敛阶：$O(h)$</li>
<li>优点：更高的收敛阶</li>
<li>缺点：需要计算二阶项，计算复杂度更高</li>
</ul>
<p><strong>随机龙格库塔方法</strong>：
</p>
$$\begin{aligned}
\mathbf{K}_1 &= \mathbf{f}(\mathbf{x}_i, t_i)h + \mathbf{G}(t_i)\Delta\mathbf{W}_i \\
\mathbf{K}_2 &= \mathbf{f}(\mathbf{x}_i + \mathbf{K}_1, t_{i+1})h + \mathbf{G}(t_{i+1})\Delta\mathbf{W}_i \\
\mathbf{x}_{i+1} &= \mathbf{x}_i + \frac{1}{2}(\mathbf{K}_1 + \mathbf{K}_2)
\end{aligned}$$<h3 id="22-扩散模型应用">2.2 扩散模型应用</h3>
<h4 id="221-前向sde">2.2.1 前向SDE</h4>
$$d\mathbf{x} = -\frac{1}{2}\beta(t)\mathbf{x}dt + \sqrt{\beta(t)}d\mathbf{W}$$<p><strong>解析解</strong>：
</p>
$$\mathbf{x}(t) = \mathbf{x}(0)e^{-\frac{1}{2}\int_0^t \beta(s)ds} + \int_0^t e^{-\frac{1}{2}\int_s^t \beta(\tau)d\tau}\sqrt{\beta(s)}d\mathbf{W}(s)$$<p><strong>数值实现</strong>：
对于线性SDE，可以使用精确的解析解，避免数值误差。</p>
<h4 id="222-逆向sde">2.2.2 逆向SDE</h4>
$$d\mathbf{x} = \left[\mathbf{f}(\mathbf{x}, t) - \mathbf{G}(t)\mathbf{G}(t)^T s_\theta(\mathbf{x}, t)\right]dt + \mathbf{G}(t)d\bar{\mathbf{W}}$$<p><strong>关键考虑</strong>：</p>
<ul>
<li><strong>时间反向</strong>：$t=T \to t=0$</li>
<li><strong>分数函数估计</strong>：$s_\theta(\mathbf{x}, t)$ 需要准确估计</li>
<li><strong>逆向维纳过程</strong>：$\bar{\mathbf{W}}$</li>
</ul>
<p><strong>数值挑战</strong>：</p>
<ol>
<li><strong>分数函数精度</strong>：网络估计的分数函数存在误差</li>
<li><strong>时间反向</strong>：需要处理逆向时间过程</li>
<li><strong>随机性控制</strong>：需要平衡确定性和随机性</li>
</ol>
<h4 id="223-概率流ode">2.2.3 概率流ODE</h4>
$$\frac{d\mathbf{x}}{dt} = \mathbf{f}(\mathbf{x}, t) - \frac{1}{2}\mathbf{G}(t)\mathbf{G}(t)^T s_\theta(\mathbf{x}, t)$$<p><strong>优势</strong>：</p>
<ul>
<li>确定性过程，避免随机性</li>
<li>可以使用高阶ODE求解器</li>
<li>计算效率更高</li>
</ul>
<p><strong>挑战</strong>：</p>
<ul>
<li>失去随机性，可能影响生成质量</li>
<li>对分数函数精度要求更高</li>
</ul>
<h3 id="23-误差与稳定性">2.3 误差与稳定性</h3>
<h4 id="231-误差来源">2.3.1 误差来源</h4>
<ol>
<li>
<p><strong>截断误差</strong>：数值方法的近似误差</p>
<ul>
<li>与步长 $h$ 相关</li>
<li>可以通过高阶方法减少</li>
</ul>
</li>
<li>
<p><strong>舍入误差</strong>：浮点精度限制</p>
<ul>
<li>累积误差问题</li>
<li>需要数值稳定性分析</li>
</ul>
</li>
<li>
<p><strong>模型误差</strong>：分数函数估计</p>
<ul>
<li>网络训练误差</li>
<li>泛化误差</li>
</ul>
</li>
<li>
<p><strong>采样误差</strong>：随机性影响</p>
<ul>
<li>有限样本效应</li>
<li>可以通过多次采样减少</li>
</ul>
</li>
</ol>
<h4 id="232-误差传播">2.3.2 误差传播</h4>
$$\|\mathbf{x}(t) - \hat{\mathbf{x}}(t)\| \leq \|\mathbf{x}(0) - \hat{\mathbf{x}}(0)\|e^{Lt} + \frac{M}{L}(e^{Lt} - 1)$$<p>其中 $L$ 是Lipschitz常数，$M$ 是局部误差界。</p>
<p><strong>误差控制策略</strong>：</p>
<ol>
<li><strong>自适应步长</strong>：根据局部误差调整步长</li>
<li><strong>高阶方法</strong>：使用更高阶的数值方法</li>
<li><strong>误差估计</strong>：实时监控误差大小</li>
</ol>
<h4 id="233-稳定性">2.3.3 稳定性</h4>
<ul>
<li><strong>A稳定性</strong>：对于线性测试方程 $y&rsquo; = \lambda y$，当 $\text{Re}(\lambda) &lt; 0$ 时解收敛</li>
<li><strong>L稳定性</strong>：更强的稳定性条件</li>
<li><strong>随机稳定性</strong>：考虑随机项的稳定性</li>
</ul>
<p><strong>稳定性分析</strong>：
对于扩散模型中的线性SDE：
</p>
$$d\mathbf{x} = -\frac{1}{2}\beta(t)\mathbf{x}dt + \sqrt{\beta(t)}d\mathbf{W}$$<p>稳定性条件要求 $\beta(t) &gt; 0$，确保漂移项提供收缩力。</p>
<h3 id="24-实现优化">2.4 实现优化</h3>
<h4 id="241-步长策略">2.4.1 步长策略</h4>
<ul>
<li><strong>固定步长</strong>：简单实现，但可能效率不高</li>
<li><strong>自适应步长</strong>：根据误差自动调整，提高效率</li>
<li><strong>多尺度方法</strong>：不同时间尺度使用不同步长</li>
</ul>
<p><strong>自适应步长算法</strong>：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">adaptive_step_size</span>(error, tol, order):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> h <span style="color:#f92672">*</span> (tol <span style="color:#f92672">/</span> error) <span style="color:#f92672">**</span> (<span style="color:#ae81ff">1.0</span> <span style="color:#f92672">/</span> order)
</span></span></code></pre></div><h4 id="242-计算效率">2.4.2 计算效率</h4>
<ul>
<li><strong>并行计算</strong>：多个轨迹并行采样</li>
<li><strong>GPU加速</strong>：利用GPU并行计算能力</li>
<li><strong>内存管理</strong>：避免存储完整轨迹</li>
</ul>
<p><strong>优化策略</strong>：</p>
<ol>
<li><strong>批处理</strong>：同时处理多个样本</li>
<li><strong>内存复用</strong>：重用中间计算结果</li>
<li><strong>缓存机制</strong>：缓存重复计算的结果</li>
</ol>
<h4 id="243-数值稳定性">2.4.3 数值稳定性</h4>
<ul>
<li><strong>条件数控制</strong>：避免病态问题</li>
<li><strong>正则化</strong>：添加正则化项提高稳定性</li>
<li><strong>梯度裁剪</strong>：防止梯度爆炸</li>
</ul>
<p><strong>稳定性技巧</strong>：</p>
<ol>
<li><strong>归一化</strong>：保持数值在合理范围内</li>
<li><strong>约束</strong>：添加物理约束</li>
<li><strong>平滑化</strong>：使用平滑的激活函数</li>
</ol>
<h3 id="25-高级技术">2.5 高级技术</h3>
<h4 id="251-概率流ode">2.5.1 概率流ODE</h4>
$$\frac{d\mathbf{x}}{dt} = \mathbf{f}(\mathbf{x}, t) - \frac{1}{2}\mathbf{G}(t)\mathbf{G}(t)^T s_\theta(\mathbf{x}, t)$$<ul>
<li><strong>高阶方法</strong>：RK4、自适应方法</li>
<li><strong>隐式方法</strong>：处理刚性ODE</li>
<li><strong>辛方法</strong>：保持几何结构</li>
</ul>
<p><strong>辛方法优势</strong>：</p>
<ul>
<li>保持哈密顿结构</li>
<li>长期数值稳定性</li>
<li>能量守恒</li>
</ul>
<h4 id="252-混合方法">2.5.2 混合方法</h4>
<ul>
<li><strong>确定性部分</strong>：使用ODE求解器</li>
<li><strong>随机部分</strong>：使用SDE求解器</li>
<li><strong>自适应切换</strong>：根据问题特性选择方法</li>
</ul>
<p><strong>混合策略</strong>：</p>
<ol>
<li><strong>早期阶段</strong>：使用SDE，保持随机性</li>
<li><strong>后期阶段</strong>：切换到ODE，提高效率</li>
<li><strong>条件切换</strong>：根据误差大小决定</li>
</ol>
<h4 id="253-加速技术">2.5.3 加速技术</h4>
<p><strong>DDIM采样</strong>：
确定性采样的高效变体，支持快速采样。</p>
<p><strong>DPM-Solver</strong>：
高阶求解器，减少采样步数。</p>
<p><strong>PNDM</strong>：
伪数值方法，平衡质量和速度。</p>
<h3 id="26-总结">2.6 总结</h3>
<p>数值求解是扩散模型实现的关键：</p>
<ol>
<li><strong>方法选择</strong>：精度与效率平衡</li>
<li><strong>误差控制</strong>：多源误差管理</li>
<li><strong>稳定性保证</strong>：数值稳定性</li>
<li><strong>效率优化</strong>：计算资源利用</li>
</ol>
<p><strong>实际建议</strong>：</p>
<ul>
<li>对于简单问题，使用欧拉方法</li>
<li>对于精度要求高的问题，使用RK4或自适应方法</li>
<li>对于大规模问题，考虑并行和GPU加速</li>
<li>对于稳定性要求高的问题，使用隐式方法</li>
</ul>
<h2 id="三-smld和sde的关系">三. SMLD和SDE的关系</h2>
<p>在前面的章节中，我们讨论了SDE/ODE视角下的扩散模型理论。现在我们将这些理论与实际应用中的Score Matching with Langevin Dynamics (SMLD)方法联系起来，展示离散时间过程如何自然地过渡到连续时间SDE。</p>
<h3 id="31-smld的基本思想">3.1 SMLD的基本思想</h3>
<p>Score Matching with Langevin Dynamics (SMLD) 是一种基于分数函数的生成模型方法，其核心思想是：</p>
<ol>
<li><strong>分数学习</strong>：学习数据分布的对数概率密度梯度</li>
<li><strong>朗之万采样</strong>：使用朗之万动力学进行采样</li>
<li><strong>噪声调度</strong>：通过逐步添加噪声实现前向过程</li>
</ol>
<h4 id="311-分数函数定义">3.1.1 分数函数定义</h4>
<p>对于数据分布 $p_{\text{data}}(\mathbf{x})$，分数函数定义为：</p>
$$\nabla_{\mathbf{x}} \log p_{\text{data}}(\mathbf{x})$$<p>这个函数描述了在给定点 $\mathbf{x}$ 处，概率密度增加最快的方向。</p>
<p><strong>物理意义</strong>：</p>
<ul>
<li><strong>方向信息</strong>：指向概率密度增加的方向</li>
<li><strong>强度信息</strong>：梯度大小反映概率变化的快慢</li>
<li><strong>局部特性</strong>：只依赖当前点的局部概率结构</li>
</ul>
<p><strong>数学性质</strong>：</p>
<ol>
<li><strong>尺度不变性</strong>：对概率密度的单调变换保持不变</li>
<li><strong>线性性</strong>：$\nabla_{\mathbf{x}} \log [p_1(\mathbf{x})p_2(\mathbf{x})] = \nabla_{\mathbf{x}} \log p_1(\mathbf{x}) + \nabla_{\mathbf{x}} \log p_2(\mathbf{x})$</li>
<li><strong>链式法则</strong>：$\nabla_{\mathbf{x}} \log p(f(\mathbf{x})) = \nabla_{\mathbf{x}} f(\mathbf{x}) \cdot \nabla_{f(\mathbf{x})} \log p(f(\mathbf{x}))$</li>
</ol>
<h4 id="312-朗之万动力学">3.1.2 朗之万动力学</h4>
<p>朗之万动力学是一种随机采样方法，其更新规则为：</p>
$$\mathbf{x}_{t+1} = \mathbf{x}_t + \epsilon \nabla_{\mathbf{x}} \log p(\mathbf{x}_t) + \sqrt{2\epsilon} \boldsymbol{\eta}_t$$<p>其中 $\epsilon$ 是步长，$\boldsymbol{\eta}_t \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ 是高斯噪声。</p>
<p><strong>物理背景</strong>：
朗之万动力学起源于统计物理学，描述粒子在势场中的随机运动：</p>
<ul>
<li><strong>漂移项</strong>：$\epsilon \nabla_{\mathbf{x}} \log p(\mathbf{x}_t)$ 对应势场的梯度力</li>
<li><strong>扩散项</strong>：$\sqrt{2\epsilon} \boldsymbol{\eta}_t$ 对应热噪声</li>
</ul>
<p><strong>收敛性质</strong>：</p>
<ul>
<li><strong>稳态分布</strong>：在适当条件下，采样轨迹收敛到目标分布</li>
<li><strong>混合时间</strong>：与步长 $\epsilon$ 和分布复杂度相关</li>
<li><strong>遍历性</strong>：长时间平均等于空间平均</li>
</ul>
<h3 id="32-离散时间smld过程">3.2 离散时间SMLD过程</h3>
<h4 id="321-前向过程噪声添加">3.2.1 前向过程（噪声添加）</h4>
<p>SMLD的前向过程通过逐步添加噪声实现：</p>
$$\mathbf{x}_i = \mathbf{x}_{i-1} + \sqrt{\beta_i} \boldsymbol{\epsilon}_i$$<p>其中 $\beta_i$ 是噪声调度，$\boldsymbol{\epsilon}_i \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$。</p>
<p><strong>噪声调度设计</strong>：</p>
<ol>
<li><strong>线性调度</strong>：$\beta_i = \beta_0 + i \cdot \Delta\beta$</li>
<li><strong>指数调度</strong>：$\beta_i = \beta_0 \cdot \alpha^i$</li>
<li><strong>余弦调度</strong>：$\beta_i = \beta_0 \cos^2(\frac{\pi i}{2N})$</li>
</ol>
<p><strong>设计原则</strong>：</p>
<ul>
<li><strong>渐进性</strong>：噪声逐渐增加，避免突变</li>
<li><strong>充分性</strong>：最终噪声足够大，覆盖整个空间</li>
<li><strong>效率性</strong>：在有限步数内完成转换</li>
</ul>
<h4 id="322-逆向过程去噪">3.2.2 逆向过程（去噪）</h4>
<p>逆向过程使用学习的分数函数进行去噪：</p>
$$\mathbf{x}_{i-1} = \mathbf{x}_i - \beta_i s_\theta(\mathbf{x}_i, i) + \sqrt{\beta_i} \boldsymbol{\eta}_i$$<p>其中 $s_\theta(\mathbf{x}_i, i)$ 是学习的分数函数。</p>
<p><strong>关键考虑</strong>：</p>
<ol>
<li><strong>分数函数精度</strong>：$s_\theta(\mathbf{x}<em>i, i)$ 需要准确估计 $\nabla</em>{\mathbf{x}} \log p_i(\mathbf{x}_i)$</li>
<li><strong>噪声控制</strong>：$\sqrt{\beta_i} \boldsymbol{\eta}_i$ 提供随机性，避免确定性轨迹</li>
<li><strong>步长选择</strong>：$\beta_i$ 需要与噪声调度匹配</li>
</ol>
<h3 id="33-从离散到连续sde极限">3.3 从离散到连续：SDE极限</h3>
<h4 id="331-离散过程的数学形式">3.3.1 离散过程的数学形式</h4>
<p>将离散时间过程写为差分方程：</p>
$$\Delta \mathbf{x}_i = \mathbf{x}_i - \mathbf{x}_{i-1} = \sqrt{\beta_i} \boldsymbol{\epsilon}_i$$<h4 id="332-连续时间极限">3.3.2 连续时间极限</h4>
<p>当时间步长 $\Delta t \to 0$ 时，离散过程收敛到连续SDE：</p>
$$d\mathbf{x} = \mathbf{f}(\mathbf{x}, t)dt + \mathbf{G}(t)d\mathbf{W}$$<p>其中：</p>
<ul>
<li>$\mathbf{f}(\mathbf{x}, t) = \mathbf{0}$（无漂移）</li>
<li>$\mathbf{G}(t) = \sqrt{\beta(t)}\mathbf{I}$（扩散矩阵）</li>
</ul>
<p><strong>收敛性分析</strong>：</p>
<ul>
<li><strong>强收敛</strong>：路径wise收敛到连续过程</li>
<li><strong>弱收敛</strong>：分布收敛到连续过程的分布</li>
<li><strong>收敛阶</strong>：通常为 $O(\Delta t^{1/2})$</li>
</ul>
<h4 id="333-噪声调度的连续化">3.3.3 噪声调度的连续化</h4>
<p>离散噪声调度 ${\beta_i}_{i=1}^N$ 对应连续噪声调度 $\beta(t)$：</p>
$$\beta(t) = \lim_{\Delta t \to 0} \frac{\beta_i}{\Delta t}$$<p><strong>连续化策略</strong>：</p>
<ol>
<li><strong>线性插值</strong>：$\beta(t) = \beta_i + (t - t_i)\frac{\beta_{i+1} - \beta_i}{\Delta t}$</li>
<li><strong>平滑插值</strong>：使用样条函数等平滑方法</li>
<li><strong>解析形式</strong>：直接设计连续函数 $\beta(t)$</li>
</ol>
<h3 id="34-smld与sde的对应关系">3.4 SMLD与SDE的对应关系</h3>
<h4 id="341-前向过程对应">3.4.1 前向过程对应</h4>
<table>
  <thead>
      <tr>
          <th>SMLD离散过程</th>
          <th>SDE连续过程</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$\mathbf{x}<em>i = \mathbf{x}</em>{i-1} + \sqrt{\beta_i} \boldsymbol{\epsilon}_i$</td>
          <td>$d\mathbf{x} = \sqrt{\beta(t)}d\mathbf{W}$</td>
      </tr>
      <tr>
          <td>噪声调度 ${\beta_i}$</td>
          <td>噪声调度 $\beta(t)$</td>
      </tr>
      <tr>
          <td>高斯噪声 $\boldsymbol{\epsilon}_i$</td>
          <td>维纳过程 $d\mathbf{W}$</td>
      </tr>
  </tbody>
</table>
<h4 id="342-逆向过程对应">3.4.2 逆向过程对应</h4>
<table>
  <thead>
      <tr>
          <th>SMLD离散过程</th>
          <th>SDE连续过程</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$\mathbf{x}_{i-1} = \mathbf{x}<em>i - \beta_i s</em>\theta(\mathbf{x}_i, i) + \sqrt{\beta_i} \boldsymbol{\eta}_i$</td>
          <td>$d\mathbf{x} = [-\beta(t)s_\theta(\mathbf{x}, t)]dt + \sqrt{\beta(t)}d\bar{\mathbf{W}}$</td>
      </tr>
      <tr>
          <td>分数函数 $s_\theta(\mathbf{x}_i, i)$</td>
          <td>分数函数 $s_\theta(\mathbf{x}, t)$</td>
      </tr>
      <tr>
          <td>步长 $\beta_i$</td>
          <td>时间微分 $dt$</td>
      </tr>
  </tbody>
</table>
<p><strong>对应关系的重要性</strong>：</p>
<ol>
<li><strong>理论统一</strong>：离散和连续过程在理论上是等价的</li>
<li><strong>实现指导</strong>：连续理论指导离散实现</li>
<li><strong>分析工具</strong>：可以使用连续理论分析离散过程</li>
</ol>
<h3 id="35-概率流ode视角">3.5 概率流ODE视角</h3>
<h4 id="351-确定性过程">3.5.1 确定性过程</h4>
<p>除了随机SDE过程，我们还可以构造确定性的概率流ODE：</p>
$$\frac{d\mathbf{x}}{dt} = \mathbf{f}(\mathbf{x}, t) - \frac{1}{2}\mathbf{G}(t)\mathbf{G}(t)^T s_\theta(\mathbf{x}, t)$$<p>对于SMLD对应的SDE：</p>
$$\frac{d\mathbf{x}}{dt} = -\frac{1}{2}\beta(t)s_\theta(\mathbf{x}, t)$$<p><strong>ODE的优势</strong>：</p>
<ol>
<li><strong>确定性</strong>：每次采样得到相同结果</li>
<li><strong>高效性</strong>：可以使用高阶ODE求解器</li>
<li><strong>可控性</strong>：更容易控制采样过程</li>
</ol>
<p><strong>ODE的挑战</strong>：</p>
<ol>
<li><strong>随机性缺失</strong>：可能影响生成多样性</li>
<li><strong>精度要求</strong>：对分数函数精度要求更高</li>
<li><strong>稳定性</strong>：需要更稳定的数值方法</li>
</ol>
<h4 id="352-ode与sde的关系">3.5.2 ODE与SDE的关系</h4>
<ul>
<li><strong>SDE</strong>：包含随机项，产生多样化的采样轨迹</li>
<li><strong>ODE</strong>：确定性过程，产生唯一的采样轨迹</li>
<li><strong>关系</strong>：ODE是SDE的期望轨迹</li>
</ul>
<p><strong>选择策略</strong>：</p>
<ul>
<li><strong>多样性优先</strong>：选择SDE采样</li>
<li><strong>效率优先</strong>：选择ODE采样</li>
<li><strong>混合策略</strong>：结合两种方法的优点</li>
</ul>
<h3 id="36-实际实现考虑">3.6 实际实现考虑</h3>
<h4 id="361-离散化策略">3.6.1 离散化策略</h4>
<p>将连续SDE离散化回离散过程：</p>
$$\mathbf{x}_{i-1} = \mathbf{x}_i - \Delta t \cdot \frac{1}{2}\beta(t_i)s_\theta(\mathbf{x}_i, t_i) + \sqrt{\Delta t \cdot \beta(t_i)} \boldsymbol{\eta}_i$$<p><strong>离散化方法</strong>：</p>
<ol>
<li><strong>欧拉离散化</strong>：最简单的离散化方法</li>
<li><strong>高阶离散化</strong>：使用更高阶的数值方法</li>
<li><strong>自适应离散化</strong>：根据误差自动调整步长</li>
</ol>
<h4 id="362-时间嵌入">3.6.2 时间嵌入</h4>
<p>分数网络需要时间信息：</p>
$$s_\theta(\mathbf{x}, t) = \text{Network}(\mathbf{x}, \text{TimeEmbedding}(t))$$<p><strong>时间嵌入方法</strong>：</p>
<ol>
<li><strong>正弦嵌入</strong>：$\text{TimeEmbedding}(t) = [\sin(\omega_1 t), \cos(\omega_1 t), \ldots, \sin(\omega_d t), \cos(\omega_d t)]$</li>
<li><strong>位置编码</strong>：类似Transformer的位置编码</li>
<li><strong>可学习嵌入</strong>：直接学习时间表示</li>
</ol>
<p><strong>嵌入设计考虑</strong>：</p>
<ul>
<li><strong>周期性</strong>：时间嵌入应该具有适当的周期性</li>
<li><strong>分辨率</strong>：不同频率捕获不同时间尺度</li>
<li><strong>维度</strong>：嵌入维度影响表达能力</li>
</ul>
<h4 id="363-噪声调度设计">3.6.3 噪声调度设计</h4>
<p>常见的噪声调度函数：</p>
<ol>
<li>
<p><strong>线性调度</strong>：$\beta(t) = \beta_0 + (\beta_T - \beta_0)t$</p>
<ul>
<li>优点：简单直观</li>
<li>缺点：可能不够平滑</li>
</ul>
</li>
<li>
<p><strong>余弦调度</strong>：$\beta(t) = \beta_0 \cos^2(\frac{\pi t}{2T})$</p>
<ul>
<li>优点：平滑，在边界处导数小</li>
<li>缺点：计算复杂度稍高</li>
</ul>
</li>
<li>
<p><strong>指数调度</strong>：$\beta(t) = \beta_0 e^{\alpha t}$</p>
<ul>
<li>优点：快速增长</li>
<li>缺点：可能增长过快</li>
</ul>
</li>
</ol>
<p><strong>调度选择原则</strong>：</p>
<ul>
<li><strong>数据特性</strong>：根据数据分布特性选择</li>
<li><strong>计算资源</strong>：考虑计算复杂度</li>
<li><strong>经验调优</strong>：通过实验确定最佳调度</li>
</ul>
<h3 id="37-理论优势">3.7 理论优势</h3>
<h4 id="371-统一框架">3.7.1 统一框架</h4>
<p>SDE/ODE视角提供了统一的理论框架：</p>
<ul>
<li><strong>离散SMLD</strong>：实际实现的基础</li>
<li><strong>连续SDE</strong>：理论分析的工具</li>
<li><strong>概率流ODE</strong>：高效采样的选择</li>
</ul>
<p><strong>框架优势</strong>：</p>
<ol>
<li><strong>理论完备性</strong>：基于坚实的数学基础</li>
<li><strong>实现灵活性</strong>：支持多种实现方式</li>
<li><strong>分析工具丰富</strong>：可以使用多种分析工具</li>
</ol>
<h4 id="372-灵活性">3.7.2 灵活性</h4>
<ul>
<li><strong>采样策略</strong>：可以选择SDE或ODE采样</li>
<li><strong>时间控制</strong>：可以精确控制采样步数</li>
<li><strong>条件生成</strong>：支持各种条件生成任务</li>
</ul>
<p><strong>灵活性体现</strong>：</p>
<ol>
<li><strong>多尺度采样</strong>：不同时间尺度使用不同策略</li>
<li><strong>自适应控制</strong>：根据生成质量调整参数</li>
<li><strong>条件扩展</strong>：容易扩展到条件生成</li>
</ol>
<h3 id="38-总结">3.8 总结</h3>
<p>SMLD和SDE的关系展示了离散时间过程如何自然地过渡到连续时间过程：</p>
<ol>
<li><strong>理论基础</strong>：SDE提供了坚实的数学基础</li>
<li><strong>实现指导</strong>：离散SMLD提供了实际实现方案</li>
<li><strong>灵活性</strong>：支持多种采样策略和优化方法</li>
<li><strong>扩展性</strong>：为更复杂的生成任务提供框架</li>
</ol>
<p><strong>实际应用建议</strong>：</p>
<ul>
<li><strong>理论研究</strong>：使用连续SDE进行分析</li>
<li><strong>实际实现</strong>：使用离散SMLD进行实现</li>
<li><strong>高效采样</strong>：使用概率流ODE进行快速采样</li>
<li><strong>条件生成</strong>：结合条件信息扩展框架</li>
</ul>
<p>这种对应关系不仅帮助我们理解扩散模型的本质，也为实际应用提供了重要的指导。</p>
<h2 id="四-扩散模型的统一框架">四. 扩散模型的统一框架</h2>
<p>在前面的章节中，我们分别讨论了SDE/ODE理论、数值求解方法以及SMLD与SDE的关系。现在我们将这些内容整合起来，构建一个统一的扩散模型理论框架，展示不同视角和方法之间的内在联系。</p>
<h3 id="41-统一框架概述">4.1 统一框架概述</h3>
<p>扩散模型的统一框架基于以下核心思想：</p>
<ol>
<li><strong>概率流视角</strong>：将生成过程视为概率分布的演化</li>
<li><strong>分数函数核心</strong>：以分数函数为桥梁连接不同方法</li>
<li><strong>时间连续性</strong>：从离散时间过程到连续时间过程的自然过渡</li>
<li><strong>采样灵活性</strong>：支持多种采样策略和优化方法</li>
</ol>
<h4 id="411-框架组成">4.1.1 框架组成</h4>
<p>统一框架包含以下主要组成部分：</p>
<ul>
<li><strong>理论基础</strong>：SDE/ODE理论、Fokker-Planck方程、分数函数理论</li>
<li><strong>数值方法</strong>：ODE/SDE求解器、自适应方法、加速技术</li>
<li><strong>实现策略</strong>：离散SMLD、连续SDE、概率流ODE</li>
<li><strong>优化技术</strong>：噪声调度、时间嵌入、网络架构</li>
</ul>
<h4 id="412-框架优势">4.1.2 框架优势</h4>
<ol>
<li><strong>理论完备性</strong>：基于坚实的数学基础</li>
<li><strong>实现灵活性</strong>：支持多种实现方式</li>
<li><strong>分析工具丰富</strong>：可以使用多种分析工具</li>
<li><strong>扩展性强</strong>：容易扩展到新的应用场景</li>
</ol>
<h3 id="42-核心数学框架">4.2 核心数学框架</h3>
<h4 id="421-前向过程">4.2.1 前向过程</h4>
<p>统一的前向SDE：</p>
$$d\mathbf{x} = \mathbf{f}(\mathbf{x}, t)dt + \mathbf{G}(t)d\mathbf{W}$$<p>其中：</p>
<ul>
<li>$\mathbf{f}(\mathbf{x}, t)$：漂移函数，描述确定性演化</li>
<li>$\mathbf{G}(t)$：扩散矩阵，描述随机扰动</li>
<li>$\mathbf{W}$：维纳过程，提供随机性</li>
</ul>
<p><strong>特殊形式</strong>：
对于标准扩散模型：
</p>
$$\mathbf{f}(\mathbf{x}, t) = -\frac{1}{2}\beta(t)\mathbf{x}, \quad \mathbf{G}(t) = \sqrt{\beta(t)}\mathbf{I}$$<h4 id="422-逆向过程">4.2.2 逆向过程</h4>
<p>统一的逆向SDE：</p>
$$d\mathbf{x} = \left[\mathbf{f}(\mathbf{x}, t) - \mathbf{G}(t)\mathbf{G}(t)^T s_\theta(\mathbf{x}, t)\right]dt + \mathbf{G}(t)d\bar{\mathbf{W}}$$<p>其中 $s_\theta(\mathbf{x}, t)$ 是学习的分数函数。</p>
<p><strong>关键洞察</strong>：</p>
<ul>
<li>逆向过程包含原始漂移项</li>
<li>分数函数提供额外的收缩力</li>
<li>随机项保持采样多样性</li>
</ul>
<h4 id="423-概率流ode">4.2.3 概率流ODE</h4>
<p>确定性版本：</p>
$$\frac{d\mathbf{x}}{dt} = \mathbf{f}(\mathbf{x}, t) - \frac{1}{2}\mathbf{G}(t)\mathbf{G}(t)^T s_\theta(\mathbf{x}, t)$$<p><strong>ODE的优势</strong>：</p>
<ul>
<li>确定性采样</li>
<li>高效数值求解</li>
<li>可控的生成过程</li>
</ul>
<h3 id="43-不同视角的统一">4.3 不同视角的统一</h3>
<h4 id="431-离散时间视角">4.3.1 离散时间视角</h4>
<p>SMLD的离散时间过程：</p>
$$\mathbf{x}_{i-1} = \mathbf{x}_i - \beta_i s_\theta(\mathbf{x}_i, i) + \sqrt{\beta_i} \boldsymbol{\eta}_i$$<p><strong>与连续时间的关系</strong>：</p>
<ul>
<li>离散过程是连续SDE的数值近似</li>
<li>步长 $\beta_i$ 对应时间微分 $dt$</li>
<li>噪声 $\boldsymbol{\eta}_i$ 对应维纳过程增量</li>
</ul>
<h4 id="432-连续时间视角">4.3.2 连续时间视角</h4>
<p>SDE的连续时间过程：</p>
$$d\mathbf{x} = -\frac{1}{2}\beta(t)s_\theta(\mathbf{x}, t)dt + \sqrt{\beta(t)}d\bar{\mathbf{W}}$$<p><strong>与离散时间的关系</strong>：</p>
<ul>
<li>连续SDE是离散过程的极限</li>
<li>时间 $t$ 对应步数 $i$</li>
<li>噪声调度 $\beta(t)$ 对应离散调度 ${\beta_i}$</li>
</ul>
<h4 id="433-概率流视角">4.3.3 概率流视角</h4>
<p>ODE的确定性过程：</p>
$$\frac{d\mathbf{x}}{dt} = -\frac{1}{2}\beta(t)s_\theta(\mathbf{x}, t)$$<p><strong>与其他视角的关系</strong>：</p>
<ul>
<li>ODE是SDE的期望轨迹</li>
<li>确定性过程，无随机性</li>
<li>高效但可能缺乏多样性</li>
</ul>
<h3 id="44-实现策略的统一">4.4 实现策略的统一</h3>
<h4 id="441-训练策略">4.4.1 训练策略</h4>
<p>统一的训练目标：</p>
$$\mathcal{L} = \mathbb{E}_{t, \mathbf{x}(t)} \left[\|\mathbf{s}_\theta(\mathbf{x}(t), t) - \nabla_{\mathbf{x}} \log p_t(\mathbf{x}(t))\|^2\right]$$<p><strong>训练考虑</strong>：</p>
<ol>
<li><strong>时间采样</strong>：从 $[0, T]$ 中随机采样时间点</li>
<li><strong>数据采样</strong>：从前向过程采样 $\mathbf{x}(t)$</li>
<li><strong>目标计算</strong>：计算真实的分数函数</li>
</ol>
<h4 id="442-采样策略">4.4.2 采样策略</h4>
<p>多种采样方法：</p>
<ol>
<li>
<p><strong>SDE采样</strong>：使用逆向SDE</p>
<ul>
<li>优点：保持随机性，生成质量高</li>
<li>缺点：计算复杂度高</li>
</ul>
</li>
<li>
<p><strong>ODE采样</strong>：使用概率流ODE</p>
<ul>
<li>优点：高效，确定性</li>
<li>缺点：可能缺乏多样性</li>
</ul>
</li>
<li>
<p><strong>混合采样</strong>：结合SDE和ODE</p>
<ul>
<li>优点：平衡质量和效率</li>
<li>缺点：实现复杂</li>
</ul>
</li>
</ol>
<h4 id="443-优化技术">4.4.3 优化技术</h4>
<ol>
<li>
<p><strong>噪声调度优化</strong>：</p>
<ul>
<li>线性调度：简单但可能不够平滑</li>
<li>余弦调度：平滑，边界处导数小</li>
<li>自适应调度：根据数据特性调整</li>
</ul>
</li>
<li>
<p><strong>时间嵌入优化</strong>：</p>
<ul>
<li>正弦嵌入：周期性，多尺度</li>
<li>位置编码：类似Transformer</li>
<li>可学习嵌入：直接优化</li>
</ul>
</li>
<li>
<p><strong>网络架构优化</strong>：</p>
<ul>
<li>U-Net：适合图像生成</li>
<li>Transformer：适合序列数据</li>
<li>混合架构：结合不同优势</li>
</ul>
</li>
</ol>
<h3 id="45-与其他生成模型的比较">4.5 与其他生成模型的比较</h3>
<h4 id="451-与gan的比较">4.5.1 与GAN的比较</h4>
<table>
  <thead>
      <tr>
          <th>特性</th>
          <th>扩散模型</th>
          <th>GAN</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>训练稳定性</td>
          <td>稳定</td>
          <td>不稳定</td>
      </tr>
      <tr>
          <td>模式崩塌</td>
          <td>无</td>
          <td>常见</td>
      </tr>
      <tr>
          <td>采样质量</td>
          <td>高</td>
          <td>可变</td>
      </tr>
      <tr>
          <td>计算效率</td>
          <td>低</td>
          <td>高</td>
      </tr>
      <tr>
          <td>理论基础</td>
          <td>坚实</td>
          <td>相对薄弱</td>
      </tr>
  </tbody>
</table>
<p><strong>优势</strong>：</p>
<ul>
<li>训练稳定性好</li>
<li>理论基础坚实</li>
<li>生成质量高</li>
</ul>
<p><strong>劣势</strong>：</p>
<ul>
<li>采样速度慢</li>
<li>计算复杂度高</li>
</ul>
<h4 id="452-与vae的比较">4.5.2 与VAE的比较</h4>
<table>
  <thead>
      <tr>
          <th>特性</th>
          <th>扩散模型</th>
          <th>VAE</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>生成质量</td>
          <td>高</td>
          <td>中等</td>
      </tr>
      <tr>
          <td>采样速度</td>
          <td>慢</td>
          <td>快</td>
      </tr>
      <tr>
          <td>潜在空间</td>
          <td>无</td>
          <td>有</td>
      </tr>
      <tr>
          <td>重构质量</td>
          <td>高</td>
          <td>中等</td>
      </tr>
      <tr>
          <td>理论基础</td>
          <td>坚实</td>
          <td>坚实</td>
      </tr>
  </tbody>
</table>
<p><strong>优势</strong>：</p>
<ul>
<li>生成质量高</li>
<li>重构质量好</li>
<li>理论完备</li>
</ul>
<p><strong>劣势</strong>：</p>
<ul>
<li>采样速度慢</li>
<li>无显式潜在空间</li>
</ul>
<h4 id="453-与flow模型的比较">4.5.3 与Flow模型的比较</h4>
<table>
  <thead>
      <tr>
          <th>特性</th>
          <th>扩散模型</th>
          <th>Flow模型</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>可逆性</td>
          <td>不可逆</td>
          <td>可逆</td>
      </tr>
      <tr>
          <td>采样速度</td>
          <td>慢</td>
          <td>快</td>
      </tr>
      <tr>
          <td>计算复杂度</td>
          <td>高</td>
          <td>中等</td>
      </tr>
      <tr>
          <td>表达能力</td>
          <td>强</td>
          <td>受架构限制</td>
      </tr>
      <tr>
          <td>训练稳定性</td>
          <td>稳定</td>
          <td>相对稳定</td>
      </tr>
  </tbody>
</table>
<p><strong>优势</strong>：</p>
<ul>
<li>表达能力更强</li>
<li>训练更稳定</li>
<li>理论更完备</li>
</ul>
<p><strong>劣势</strong>：</p>
<ul>
<li>采样速度慢</li>
<li>不可逆</li>
</ul>
<h3 id="46-实际应用案例">4.6 实际应用案例</h3>
<h4 id="461-图像生成">4.6.1 图像生成</h4>
<p><strong>应用场景</strong>：</p>
<ul>
<li>高分辨率图像生成</li>
<li>条件图像生成</li>
<li>图像编辑和修复</li>
</ul>
<p><strong>技术特点</strong>：</p>
<ul>
<li>使用U-Net架构</li>
<li>多尺度噪声调度</li>
<li>条件嵌入技术</li>
</ul>
<p><strong>成功案例</strong>：</p>
<ul>
<li>DDPM：首次展示高质量图像生成</li>
<li>DDIM：快速确定性采样</li>
<li>Stable Diffusion：大规模文本到图像生成</li>
</ul>
<h4 id="462-音频生成">4.6.2 音频生成</h4>
<p><strong>应用场景</strong>：</p>
<ul>
<li>音乐生成</li>
<li>语音合成</li>
<li>音频编辑</li>
</ul>
<p><strong>技术特点</strong>：</p>
<ul>
<li>使用Transformer架构</li>
<li>时间序列建模</li>
<li>多模态融合</li>
</ul>
<p><strong>成功案例</strong>：</p>
<ul>
<li>AudioCraft：高质量音频生成</li>
<li>MusicLM：文本到音乐生成</li>
<li>AudioLDM：音频扩散模型</li>
</ul>
<h4 id="463-文本生成">4.6.3 文本生成</h4>
<p><strong>应用场景</strong>：</p>
<ul>
<li>文本续写</li>
<li>风格转换</li>
<li>对话生成</li>
</ul>
<p><strong>技术特点</strong>：</p>
<ul>
<li>使用Transformer架构</li>
<li>离散化处理</li>
<li>条件生成</li>
</ul>
<p><strong>成功案例</strong>：</p>
<ul>
<li>Diffusion-LM：离散扩散语言模型</li>
<li>CDCD：连续扩散对话生成</li>
<li>DiffuSeq：序列扩散模型</li>
</ul>
<h3 id="47-理论扩展">4.7 理论扩展</h3>
<h4 id="471-条件生成">4.7.1 条件生成</h4>
<p>条件扩散模型：</p>
$$d\mathbf{x} = \mathbf{f}(\mathbf{x}, \mathbf{c}, t)dt + \mathbf{G}(t)d\mathbf{W}$$<p>其中 $\mathbf{c}$ 是条件信息。</p>
<p><strong>条件嵌入方法</strong>：</p>
<ol>
<li><strong>交叉注意力</strong>：条件与状态交互</li>
<li><strong>特征融合</strong>：直接特征拼接</li>
<li><strong>调制技术</strong>：条件调制网络参数</li>
</ol>
<h4 id="472-多模态生成">4.7.2 多模态生成</h4>
<p>多模态扩散模型：</p>
$$d\mathbf{x}_1 = \mathbf{f}_1(\mathbf{x}_1, \mathbf{x}_2, t)dt + \mathbf{G}_1(t)d\mathbf{W}_1$$<p>
</p>
$$d\mathbf{x}_2 = \mathbf{f}_2(\mathbf{x}_1, \mathbf{x}_2, t)dt + \mathbf{G}_2(t)d\mathbf{W}_2$$<p><strong>多模态融合</strong>：</p>
<ul>
<li>交叉模态注意力</li>
<li>共享潜在空间</li>
<li>模态特定编码器</li>
</ul>
<h4 id="473-可控生成">4.7.3 可控生成</h4>
<p>可控扩散模型：</p>
$$d\mathbf{x} = \mathbf{f}(\mathbf{x}, \mathbf{a}, t)dt + \mathbf{G}(t)d\mathbf{W}$$<p>其中 $\mathbf{a}$ 是控制信号。</p>
<p><strong>控制方法</strong>：</p>
<ul>
<li>梯度引导</li>
<li>分类器引导</li>
<li>能量函数引导</li>
</ul>
<h3 id="48-未来发展方向">4.8 未来发展方向</h3>
<h4 id="481-理论发展">4.8.1 理论发展</h4>
<ol>
<li>
<p><strong>更高效的采样</strong>：</p>
<ul>
<li>减少采样步数</li>
<li>提高采样质量</li>
<li>自适应采样策略</li>
</ul>
</li>
<li>
<p><strong>更好的理论理解</strong>：</p>
<ul>
<li>收敛性分析</li>
<li>误差界估计</li>
<li>稳定性理论</li>
</ul>
</li>
<li>
<p><strong>新的数学工具</strong>：</p>
<ul>
<li>几何方法</li>
<li>最优传输理论</li>
<li>信息几何</li>
</ul>
</li>
</ol>
<h4 id="482-技术发展">4.8.2 技术发展</h4>
<ol>
<li>
<p><strong>架构创新</strong>：</p>
<ul>
<li>更高效的网络架构</li>
<li>注意力机制优化</li>
<li>多尺度建模</li>
</ul>
</li>
<li>
<p><strong>训练优化</strong>：</p>
<ul>
<li>更稳定的训练策略</li>
<li>更好的损失函数</li>
<li>自适应学习率</li>
</ul>
</li>
<li>
<p><strong>应用扩展</strong>：</p>
<ul>
<li>更多模态支持</li>
<li>更大规模应用</li>
<li>实时应用</li>
</ul>
</li>
</ol>
<h4 id="483-应用发展">4.8.3 应用发展</h4>
<ol>
<li>
<p><strong>创意应用</strong>：</p>
<ul>
<li>艺术创作</li>
<li>设计辅助</li>
<li>内容生成</li>
</ul>
</li>
<li>
<p><strong>科学应用</strong>：</p>
<ul>
<li>分子设计</li>
<li>材料发现</li>
<li>药物设计</li>
</ul>
</li>
<li>
<p><strong>工业应用</strong>：</p>
<ul>
<li>产品设计</li>
<li>质量控制</li>
<li>预测建模</li>
</ul>
</li>
</ol>
<h3 id="49-总结">4.9 总结</h3>
<p>扩散模型的统一框架提供了一个强大而灵活的理论基础：</p>
<ol>
<li><strong>理论完备性</strong>：基于坚实的数学基础，包括SDE/ODE理论、概率论和数值分析</li>
<li><strong>实现灵活性</strong>：支持多种实现策略，从离散SMLD到连续SDE再到概率流ODE</li>
<li><strong>应用广泛性</strong>：适用于图像、音频、文本等多种模态的生成任务</li>
<li><strong>扩展性强</strong>：容易扩展到条件生成、多模态生成、可控生成等新应用</li>
</ol>
<p><strong>核心价值</strong>：</p>
<ul>
<li><strong>统一理解</strong>：将不同方法统一在一个理论框架下</li>
<li><strong>实践指导</strong>：为实际应用提供清晰的指导原则</li>
<li><strong>发展基础</strong>：为未来的理论和技术发展提供基础</li>
</ul>
<p><strong>实际建议</strong>：</p>
<ul>
<li><strong>理论研究</strong>：深入理解SDE/ODE理论和分数函数理论</li>
<li><strong>实践应用</strong>：根据具体需求选择合适的实现策略</li>
<li><strong>持续学习</strong>：关注最新的理论和技术发展</li>
</ul>
<p>这个统一框架不仅帮助我们理解扩散模型的本质，也为实际应用和未来发展提供了重要的指导。随着理论和技术的不断发展，扩散模型将在更多领域发挥重要作用。</p>
<h2 id="附录python实现示例">附录：Python实现示例</h2>
<p>为了更好地理解扩散模型的核心概念，我们提供一个完整的Python实现示例，展示SDE/ODE视角下的扩散模型。</p>
<h3 id="a1-基础设置">A.1 基础设置</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.optim <span style="color:#66d9ef">as</span> optim
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> torch.utils.data <span style="color:#f92672">import</span> DataLoader, TensorDataset
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> scipy.integrate <span style="color:#f92672">import</span> odeint
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> seaborn <span style="color:#66d9ef">as</span> sns
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 设置随机种子</span>
</span></span><span style="display:flex;"><span>np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>seed(<span style="color:#ae81ff">42</span>)
</span></span><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>manual_seed(<span style="color:#ae81ff">42</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 设置绘图样式</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>style<span style="color:#f92672">.</span>use(<span style="color:#e6db74">&#39;seaborn-v0_8&#39;</span>)
</span></span><span style="display:flex;"><span>sns<span style="color:#f92672">.</span>set_palette(<span style="color:#e6db74">&#34;husl&#34;</span>)
</span></span></code></pre></div><h3 id="a2-噪声调度函数">A.2 噪声调度函数</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">linear_beta_schedule</span>(t, beta_start<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0001</span>, beta_end<span style="color:#f92672">=</span><span style="color:#ae81ff">0.02</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;线性噪声调度&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> beta_start <span style="color:#f92672">+</span> (beta_end <span style="color:#f92672">-</span> beta_start) <span style="color:#f92672">*</span> t
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">cosine_beta_schedule</span>(t, beta_start<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0001</span>, beta_end<span style="color:#f92672">=</span><span style="color:#ae81ff">0.02</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;余弦噪声调度&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> beta_start <span style="color:#f92672">+</span> (beta_end <span style="color:#f92672">-</span> beta_start) <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>cos(np<span style="color:#f92672">.</span>pi <span style="color:#f92672">*</span> t <span style="color:#f92672">/</span> <span style="color:#ae81ff">2</span>) <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">exponential_beta_schedule</span>(t, beta_start<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0001</span>, beta_end<span style="color:#f92672">=</span><span style="color:#ae81ff">0.02</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;指数噪声调度&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> beta_start <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>exp(np<span style="color:#f92672">.</span>log(beta_end <span style="color:#f92672">/</span> beta_start) <span style="color:#f92672">*</span> t)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 可视化不同的噪声调度</span>
</span></span><span style="display:flex;"><span>t <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linspace(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1000</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">12</span>, <span style="color:#ae81ff">4</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(t, linear_beta_schedule(t))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Linear Schedule&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;Time&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;β(t)&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(t, cosine_beta_schedule(t))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Cosine Schedule&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;Time&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(t, exponential_beta_schedule(t))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Exponential Schedule&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;Time&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>tight_layout()
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><h3 id="a3-前向过程实现">A.3 前向过程实现</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">ForwardProcess</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;前向扩散过程&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, beta_schedule<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;linear&#39;</span>, T<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>T <span style="color:#f92672">=</span> T
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>t <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linspace(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, T)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> beta_schedule <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;linear&#39;</span>:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>beta <span style="color:#f92672">=</span> linear_beta_schedule(self<span style="color:#f92672">.</span>t)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">elif</span> beta_schedule <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;cosine&#39;</span>:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>beta <span style="color:#f92672">=</span> cosine_beta_schedule(self<span style="color:#f92672">.</span>t)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">elif</span> beta_schedule <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;exponential&#39;</span>:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>beta <span style="color:#f92672">=</span> exponential_beta_schedule(self<span style="color:#f92672">.</span>t)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 预计算累积量</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> self<span style="color:#f92672">.</span>beta
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>alpha_bar <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>cumprod(self<span style="color:#f92672">.</span>alpha)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>sqrt_alpha_bar <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sqrt(self<span style="color:#f92672">.</span>alpha_bar)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>sqrt_one_minus_alpha_bar <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sqrt(<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> self<span style="color:#f92672">.</span>alpha_bar)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sample_forward</span>(self, x0, t):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;从x0采样xt&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> isinstance(t, int):
</span></span><span style="display:flex;"><span>            t <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([t])
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        noise <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(<span style="color:#f92672">*</span>x0<span style="color:#f92672">.</span>shape)
</span></span><span style="display:flex;"><span>        sqrt_alpha_bar_t <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>sqrt_alpha_bar[t]
</span></span><span style="display:flex;"><span>        sqrt_one_minus_alpha_bar_t <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>sqrt_one_minus_alpha_bar[t]
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        xt <span style="color:#f92672">=</span> sqrt_alpha_bar_t<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>) <span style="color:#f92672">*</span> x0 <span style="color:#f92672">+</span> sqrt_one_minus_alpha_bar_t<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>) <span style="color:#f92672">*</span> noise
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> xt, noise
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_xt_distribution</span>(self, x0, t):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;获取xt的分布参数&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        mean <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>sqrt_alpha_bar[t] <span style="color:#f92672">*</span> x0
</span></span><span style="display:flex;"><span>        std <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>sqrt_one_minus_alpha_bar[t]
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> mean, std
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 测试前向过程</span>
</span></span><span style="display:flex;"><span>forward_process <span style="color:#f92672">=</span> ForwardProcess(beta_schedule<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;cosine&#39;</span>, T<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 生成一些测试数据</span>
</span></span><span style="display:flex;"><span>x0 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">100</span>, <span style="color:#ae81ff">2</span>)  <span style="color:#75715e"># 2D数据</span>
</span></span><span style="display:flex;"><span>t_steps <span style="color:#f92672">=</span> [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">100</span>, <span style="color:#ae81ff">500</span>, <span style="color:#ae81ff">999</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">12</span>, <span style="color:#ae81ff">3</span>))
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i, t <span style="color:#f92672">in</span> enumerate(t_steps):
</span></span><span style="display:flex;"><span>    xt, _ <span style="color:#f92672">=</span> forward_process<span style="color:#f92672">.</span>sample_forward(x0, t)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">4</span>, i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>scatter(xt[:, <span style="color:#ae81ff">0</span>], xt[:, <span style="color:#ae81ff">1</span>], alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.6</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;t = </span><span style="color:#e6db74">{</span>t<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>xlim(<span style="color:#f92672">-</span><span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>ylim(<span style="color:#f92672">-</span><span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>tight_layout()
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><h3 id="a4-分数网络实现">A.4 分数网络实现</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">ScoreNetwork</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;分数网络&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, input_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, hidden_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, time_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 时间嵌入</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>time_embed <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">1</span>, time_dim),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>SiLU(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(time_dim, time_dim)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 主网络</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>net <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(input_dim <span style="color:#f92672">+</span> time_dim, hidden_dim),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>SiLU(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(hidden_dim, hidden_dim),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>SiLU(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(hidden_dim, hidden_dim),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>SiLU(),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Linear(hidden_dim, input_dim)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, t):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 时间嵌入</span>
</span></span><span style="display:flex;"><span>        t_emb <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>time_embed(t<span style="color:#f92672">.</span>unsqueeze(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 拼接输入</span>
</span></span><span style="display:flex;"><span>        x_t <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([x, t_emb], dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 前向传播</span>
</span></span><span style="display:flex;"><span>        score <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>net(x_t)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> score
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 测试分数网络</span>
</span></span><span style="display:flex;"><span>score_net <span style="color:#f92672">=</span> ScoreNetwork()
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>t <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>rand(<span style="color:#ae81ff">10</span>)
</span></span><span style="display:flex;"><span>score <span style="color:#f92672">=</span> score_net(x, t)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Score shape: </span><span style="color:#e6db74">{</span>score<span style="color:#f92672">.</span>shape<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><h3 id="a5-训练函数">A.5 训练函数</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">train_score_network</span>(score_net, forward_process, data, epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>, lr<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-3</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;训练分数网络&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    optimizer <span style="color:#f92672">=</span> optim<span style="color:#f92672">.</span>Adam(score_net<span style="color:#f92672">.</span>parameters(), lr<span style="color:#f92672">=</span>lr)
</span></span><span style="display:flex;"><span>    criterion <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>MSELoss()
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    losses <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(epochs):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 随机采样时间步</span>
</span></span><span style="display:flex;"><span>        t <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randint(<span style="color:#ae81ff">0</span>, forward_process<span style="color:#f92672">.</span>T, size<span style="color:#f92672">=</span>data<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 前向过程采样</span>
</span></span><span style="display:flex;"><span>        xt, noise <span style="color:#f92672">=</span> forward_process<span style="color:#f92672">.</span>sample_forward(data, t)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 转换为tensor</span>
</span></span><span style="display:flex;"><span>        xt_tensor <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>FloatTensor(xt)
</span></span><span style="display:flex;"><span>        t_tensor <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>FloatTensor(t <span style="color:#f92672">/</span> forward_process<span style="color:#f92672">.</span>T)  <span style="color:#75715e"># 归一化到[0,1]</span>
</span></span><span style="display:flex;"><span>        noise_tensor <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>FloatTensor(noise)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 预测分数</span>
</span></span><span style="display:flex;"><span>        predicted_score <span style="color:#f92672">=</span> score_net(xt_tensor, t_tensor)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 真实分数（对于高斯噪声，分数就是噪声的负值）</span>
</span></span><span style="display:flex;"><span>        true_score <span style="color:#f92672">=</span> <span style="color:#f92672">-</span>noise_tensor <span style="color:#f92672">/</span> forward_process<span style="color:#f92672">.</span>sqrt_one_minus_alpha_bar[t]<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 计算损失</span>
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> criterion(predicted_score, true_score)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 反向传播</span>
</span></span><span style="display:flex;"><span>        optimizer<span style="color:#f92672">.</span>zero_grad()
</span></span><span style="display:flex;"><span>        loss<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>        optimizer<span style="color:#f92672">.</span>step()
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        losses<span style="color:#f92672">.</span>append(loss<span style="color:#f92672">.</span>item())
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> epoch <span style="color:#f92672">%</span> <span style="color:#ae81ff">100</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>            print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Epoch </span><span style="color:#e6db74">{</span>epoch<span style="color:#e6db74">}</span><span style="color:#e6db74">, Loss: </span><span style="color:#e6db74">{</span>loss<span style="color:#f92672">.</span>item()<span style="color:#e6db74">:</span><span style="color:#e6db74">.6f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> losses
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 生成训练数据（混合高斯分布）</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">generate_mixture_gaussian_data</span>(n_samples<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;生成混合高斯分布数据&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    n_components <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span>
</span></span><span style="display:flex;"><span>    means <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>], [<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>], [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">2</span>]])
</span></span><span style="display:flex;"><span>    covs <span style="color:#f92672">=</span> [np<span style="color:#f92672">.</span>eye(<span style="color:#ae81ff">2</span>) <span style="color:#f92672">*</span> <span style="color:#ae81ff">0.1</span> <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(n_components)]
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    data <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(n_samples):
</span></span><span style="display:flex;"><span>        component <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randint(<span style="color:#ae81ff">0</span>, n_components)
</span></span><span style="display:flex;"><span>        sample <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>multivariate_normal(means[component], covs[component])
</span></span><span style="display:flex;"><span>        data<span style="color:#f92672">.</span>append(sample)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>array(data)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 训练模型</span>
</span></span><span style="display:flex;"><span>data <span style="color:#f92672">=</span> generate_mixture_gaussian_data(<span style="color:#ae81ff">1000</span>)
</span></span><span style="display:flex;"><span>score_net <span style="color:#f92672">=</span> ScoreNetwork()
</span></span><span style="display:flex;"><span>losses <span style="color:#f92672">=</span> train_score_network(score_net, forward_process, data, epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 绘制训练损失</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">4</span>))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(losses)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Training Loss&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;Epoch&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;Loss&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 可视化训练数据</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>scatter(data[:, <span style="color:#ae81ff">0</span>], data[:, <span style="color:#ae81ff">1</span>], alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.6</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Training Data&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>tight_layout()
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><h3 id="a6-采样函数">A.6 采样函数</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sample_sde</span>(score_net, forward_process, n_samples<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>, n_steps<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;使用SDE采样&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 从噪声开始</span>
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(n_samples, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 时间步长</span>
</span></span><span style="display:flex;"><span>    dt <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span> <span style="color:#f92672">/</span> n_steps
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    samples <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(n_steps):
</span></span><span style="display:flex;"><span>        t <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span> <span style="color:#f92672">-</span> i <span style="color:#f92672">*</span> dt  <span style="color:#75715e"># 反向时间</span>
</span></span><span style="display:flex;"><span>        t_tensor <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>full((n_samples,), t)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 计算分数</span>
</span></span><span style="display:flex;"><span>        score <span style="color:#f92672">=</span> score_net(x, t_tensor)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># SDE更新</span>
</span></span><span style="display:flex;"><span>        beta_t <span style="color:#f92672">=</span> forward_process<span style="color:#f92672">.</span>beta[int(t <span style="color:#f92672">*</span> (forward_process<span style="color:#f92672">.</span>T <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>))]
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 漂移项</span>
</span></span><span style="display:flex;"><span>        drift <span style="color:#f92672">=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">0.5</span> <span style="color:#f92672">*</span> beta_t <span style="color:#f92672">*</span> score <span style="color:#f92672">*</span> dt
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 扩散项</span>
</span></span><span style="display:flex;"><span>        diffusion <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>sqrt(torch<span style="color:#f92672">.</span>tensor(beta_t <span style="color:#f92672">*</span> dt)) <span style="color:#f92672">*</span> torch<span style="color:#f92672">.</span>randn_like(x)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 更新x</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x <span style="color:#f92672">+</span> drift <span style="color:#f92672">+</span> diffusion
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> i <span style="color:#f92672">%</span> <span style="color:#ae81ff">100</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>            samples<span style="color:#f92672">.</span>append(x<span style="color:#f92672">.</span>detach()<span style="color:#f92672">.</span>numpy())
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> samples
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sample_ode</span>(score_net, forward_process, n_samples<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>, n_steps<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;使用ODE采样&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 从噪声开始</span>
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(n_samples, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 时间步长</span>
</span></span><span style="display:flex;"><span>    dt <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span> <span style="color:#f92672">/</span> n_steps
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    samples <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(n_steps):
</span></span><span style="display:flex;"><span>        t <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span> <span style="color:#f92672">-</span> i <span style="color:#f92672">*</span> dt  <span style="color:#75715e"># 反向时间</span>
</span></span><span style="display:flex;"><span>        t_tensor <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>full((n_samples,), t)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 计算分数</span>
</span></span><span style="display:flex;"><span>        score <span style="color:#f92672">=</span> score_net(x, t_tensor)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># ODE更新（无随机项）</span>
</span></span><span style="display:flex;"><span>        beta_t <span style="color:#f92672">=</span> forward_process<span style="color:#f92672">.</span>beta[int(t <span style="color:#f92672">*</span> (forward_process<span style="color:#f92672">.</span>T <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>))]
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x <span style="color:#f92672">-</span> <span style="color:#ae81ff">0.5</span> <span style="color:#f92672">*</span> beta_t <span style="color:#f92672">*</span> score <span style="color:#f92672">*</span> dt
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> i <span style="color:#f92672">%</span> <span style="color:#ae81ff">100</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>            samples<span style="color:#f92672">.</span>append(x<span style="color:#f92672">.</span>detach()<span style="color:#f92672">.</span>numpy())
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> samples
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 采样</span>
</span></span><span style="display:flex;"><span>sde_samples <span style="color:#f92672">=</span> sample_sde(score_net, forward_process, n_samples<span style="color:#f92672">=</span><span style="color:#ae81ff">500</span>, n_steps<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>)
</span></span><span style="display:flex;"><span>ode_samples <span style="color:#f92672">=</span> sample_ode(score_net, forward_process, n_samples<span style="color:#f92672">=</span><span style="color:#ae81ff">500</span>, n_steps<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 可视化采样结果</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">15</span>, <span style="color:#ae81ff">5</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 原始数据</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>scatter(data[:, <span style="color:#ae81ff">0</span>], data[:, <span style="color:#ae81ff">1</span>], alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.6</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Original Data&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Original Data&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>legend()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># SDE采样</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>scatter(sde_samples[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>][:, <span style="color:#ae81ff">0</span>], sde_samples[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>][:, <span style="color:#ae81ff">1</span>], alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.6</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;SDE Samples&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;SDE Sampling&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>legend()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ODE采样</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>scatter(ode_samples[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>][:, <span style="color:#ae81ff">0</span>], ode_samples[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>][:, <span style="color:#ae81ff">1</span>], alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.6</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;ODE Samples&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;ODE Sampling&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>legend()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>tight_layout()
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><h3 id="a7-采样过程可视化">A.7 采样过程可视化</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">visualize_sampling_process</span>(samples, title):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;可视化采样过程&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    fig, axes <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">5</span>, figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">20</span>, <span style="color:#ae81ff">8</span>))
</span></span><span style="display:flex;"><span>    axes <span style="color:#f92672">=</span> axes<span style="color:#f92672">.</span>flatten()
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 选择不同的时间步</span>
</span></span><span style="display:flex;"><span>    time_steps <span style="color:#f92672">=</span> [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">9</span>]
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i, step <span style="color:#f92672">in</span> enumerate(time_steps):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> step <span style="color:#f92672">&lt;</span> len(samples):
</span></span><span style="display:flex;"><span>            axes[i]<span style="color:#f92672">.</span>scatter(samples[step][:, <span style="color:#ae81ff">0</span>], samples[step][:, <span style="color:#ae81ff">1</span>], alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.6</span>)
</span></span><span style="display:flex;"><span>            axes[i]<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;Step </span><span style="color:#e6db74">{</span>step <span style="color:#f92672">*</span> <span style="color:#ae81ff">100</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>)
</span></span><span style="display:flex;"><span>            axes[i]<span style="color:#f92672">.</span>set_xlim(<span style="color:#f92672">-</span><span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span>)
</span></span><span style="display:flex;"><span>            axes[i]<span style="color:#f92672">.</span>set_ylim(<span style="color:#f92672">-</span><span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>suptitle(title)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>tight_layout()
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>show()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 可视化SDE和ODE采样过程</span>
</span></span><span style="display:flex;"><span>visualize_sampling_process(sde_samples, <span style="color:#e6db74">&#39;SDE Sampling Process&#39;</span>)
</span></span><span style="display:flex;"><span>visualize_sampling_process(ode_samples, <span style="color:#e6db74">&#39;ODE Sampling Process&#39;</span>)
</span></span></code></pre></div><h3 id="a8-性能分析">A.8 性能分析</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">analyze_sampling_quality</span>(original_data, samples, method_name):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;分析采样质量&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 计算分布统计</span>
</span></span><span style="display:flex;"><span>    original_mean <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>mean(original_data, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>    original_std <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>std(original_data, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    sample_mean <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>mean(samples, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>    sample_std <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>std(samples, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 计算KL散度（简化版本）</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">kl_divergence</span>(p, q):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>sum(p <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>log(p <span style="color:#f92672">/</span> q <span style="color:#f92672">+</span> <span style="color:#ae81ff">1e-10</span>))
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 计算分布相似度</span>
</span></span><span style="display:flex;"><span>    mean_diff <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>norm(original_mean <span style="color:#f92672">-</span> sample_mean)
</span></span><span style="display:flex;"><span>    std_diff <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>norm(original_std <span style="color:#f92672">-</span> sample_std)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">{</span>method_name<span style="color:#e6db74">}</span><span style="color:#e6db74"> Analysis:&#34;</span>)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Mean difference: </span><span style="color:#e6db74">{</span>mean_diff<span style="color:#e6db74">:</span><span style="color:#e6db74">.4f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Std difference: </span><span style="color:#e6db74">{</span>std_diff<span style="color:#e6db74">:</span><span style="color:#e6db74">.4f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Original mean: </span><span style="color:#e6db74">{</span>original_mean<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Sample mean: </span><span style="color:#e6db74">{</span>sample_mean<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Original std: </span><span style="color:#e6db74">{</span>original_std<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Sample std: </span><span style="color:#e6db74">{</span>sample_std<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 分析采样质量</span>
</span></span><span style="display:flex;"><span>analyze_sampling_quality(data, sde_samples[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>], <span style="color:#e6db74">&#34;SDE&#34;</span>)
</span></span><span style="display:flex;"><span>analyze_sampling_quality(data, ode_samples[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>], <span style="color:#e6db74">&#34;ODE&#34;</span>)
</span></span></code></pre></div><h3 id="a9-总结">A.9 总结</h3>
<p>这个Python实现展示了扩散模型的核心概念：</p>
<ol>
<li><strong>前向过程</strong>：通过噪声调度逐步添加噪声</li>
<li><strong>分数学习</strong>：训练网络预测分数函数</li>
<li><strong>逆向采样</strong>：使用SDE或ODE进行采样</li>
<li><strong>质量评估</strong>：分析生成样本的质量</li>
</ol>
<p><strong>关键观察</strong>：</p>
<ul>
<li>SDE采样保持随机性，生成多样化样本</li>
<li>ODE采样更高效，但可能缺乏多样性</li>
<li>分数网络的学习质量直接影响生成质量</li>
<li>噪声调度的选择影响训练和采样效果</li>
</ul>
<p>这个实现为理解扩散模型提供了实用的代码基础，可以进一步扩展到更复杂的应用场景。</p>
<ul class="pa0">
  
   <li class="list di">
     <a href="/tags/diffusion-models/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">Diffusion-Models</a>
   </li>
  
   <li class="list di">
     <a href="/tags/deep-learning/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">Deep-Learning</a>
   </li>
  
   <li class="list di">
     <a href="/tags/generative-ai/" class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">Generative-AI</a>
   </li>
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




  <div class="bg-light-gray pa3 nested-list-reset nested-copy-line-height nested-links">
    <p class="f5 b mb3">Related</p>
    <ul class="pa0 list">
	   
	     <li  class="mb2">
          <a href="/posts/diffusion-based-generative-models-4/">Diffusion-Based Generative Models &lt;4&gt;: Fokker-Planck方程</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/posts/diffusion-based-generative-models-3/">Diffusion-Based Generative Models &lt;3&gt;: SMLD</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/posts/diffusion-based-generative-models-2/">Diffusion-Based Generative Models &lt;2&gt;: DDIM</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/posts/diffusion-based-generative-models-1/">Diffusion-Based Generative Models &lt;1&gt;: DDPM</a>
        </li>
	    
    </ul>
</div>

</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white white-70 dn dib-ns pv2 ph3 no-underline" href="http://localhost:1313/" >
    &copy;  Renjie's log 2025 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

  </body>
</html>
