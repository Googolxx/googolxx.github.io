<!DOCTYPE html>
<html lang="zh-cn">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Test | Renjie&#39;s log</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="正文
注意这里可能会有歧义，
$$
\begin{equation}
\begin{aligned}
p_{\theta}(x) &= \frac{p_{\theta}(X, z)}{p(z|X)} \\
    &= \frac{P_{\theta}(X|z)p(z)}{p(z|X)}
\end{aligned}
\end{equation}
$$(注意这里可能会有歧义，我用 $p(z|X)$替代了 $p_{\theta}(z|X)$，但最后都会优化掉)。因为 $ p(z) \sim \mathcal{N}(z|0, I) $，而 $p_{\theta}(X|z)$ 就是Decoder所生成的分布，如果知道真实后验分布 $p(z|x)$，那我们也可以直接优化目标函数。但核心 $p(z|x)$ 是untracble的 （当然更严谨一点讲，也可以用hybird MC等方式来逼近，但就不在这里的讨论范畴了）。
于是在VAE中，我们可以用变分贝叶斯，引入一个Encoder，生成 $ q_{\phi}(z|X) \sim \mathcal{N}(z|\mu(X;\phi), \sigma(X;\phi)I)$ 来逼近真实后验分布 $p(z|X)$ （类似地，这里协方差矩阵也为对角矩阵）。重新推导目标函数
$$
\begin{equation}
\begin{aligned}
\log p_{\theta}(X) &\approx \frac{1}{m} \sum_{i=0}^{m} p_{\theta}(X|z_{i}) \\
    &= \frac{1}{m} \sum_{i=0}^{m} \log \left( \frac{1}{(2\pi)^{K/2} |\Sigma|^{1/2}} \exp\left(-\frac{1}{2} (X - f(z_i))^T \Sigma^{-1} (X - f(z_i))\right) \right) \\
    &= \frac{1}{m} \sum_{i=0}^{m} \left[ -\frac{K}{2} \log(2\pi) - \frac{1}{2} \log |\Sigma| - \frac{1}{2} (X - f(z_i))^T \Sigma^{-1} (X - f(z_i)) \right] \\
    &\propto \frac{1}{m} \sum_{i=0}^{m} - \frac{1}{2} (X - f(z_i))^T \Sigma^{-1} (X - f(z_i)) \\ 
    &\propto \frac{1}{m} \sum_{i=0}^{m} \sum_{k=0}^{K} \frac{\left( x^{(k)} - f(z_i;\theta)^{(k)} \right)^{2}}{\sigma^{(k)}} \\
    &\propto \frac{1}{m} \sum_{i=0}^{m} \sum_{k=0}^{K} \left( x^{(k)} - f(z_i;\theta)^{(k)} \right)^{2}
\end{aligned}
\end{equation}
$$因为真实后验分布 $p(z|X)$ 没有解析解，且KL散度这一项 $ D_{\text{KL}}(q_{\phi}(z|X) | p(z|X)) $ 始终是大于0的，因此目标优化函数可以改为最大化 第一项。在变分贝叶斯方法中，这个损失函数被称为变分下界或证据下界（variational lower bound, or evidence lower bound）">
    <meta name="generator" content="Hugo 0.146.5">
    
    
    
      <meta name="robots" content="noindex, nofollow">
    
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



  


    


    
      

    

    

    
      <link rel="canonical" href="http://localhost:1313/posts/test/">
    

    <meta property="og:url" content="http://localhost:1313/posts/test/">
  <meta property="og:site_name" content="Renjie&#39;s log">
  <meta property="og:title" content="Test">
  <meta property="og:description" content="正文 注意这里可能会有歧义，
$$ \begin{equation} \begin{aligned} p_{\theta}(x) &amp;= \frac{p_{\theta}(X, z)}{p(z|X)} \\ &amp;= \frac{P_{\theta}(X|z)p(z)}{p(z|X)} \end{aligned} \end{equation} $$(注意这里可能会有歧义，我用 $p(z|X)$替代了 $p_{\theta}(z|X)$，但最后都会优化掉)。因为 $ p(z) \sim \mathcal{N}(z|0, I) $，而 $p_{\theta}(X|z)$ 就是Decoder所生成的分布，如果知道真实后验分布 $p(z|x)$，那我们也可以直接优化目标函数。但核心 $p(z|x)$ 是untracble的 （当然更严谨一点讲，也可以用hybird MC等方式来逼近，但就不在这里的讨论范畴了）。
于是在VAE中，我们可以用变分贝叶斯，引入一个Encoder，生成 $ q_{\phi}(z|X) \sim \mathcal{N}(z|\mu(X;\phi), \sigma(X;\phi)I)$ 来逼近真实后验分布 $p(z|X)$ （类似地，这里协方差矩阵也为对角矩阵）。重新推导目标函数
$$ \begin{equation} \begin{aligned} \log p_{\theta}(X) &amp;\approx \frac{1}{m} \sum_{i=0}^{m} p_{\theta}(X|z_{i}) \\ &amp;= \frac{1}{m} \sum_{i=0}^{m} \log \left( \frac{1}{(2\pi)^{K/2} |\Sigma|^{1/2}} \exp\left(-\frac{1}{2} (X - f(z_i))^T \Sigma^{-1} (X - f(z_i))\right) \right) \\ &amp;= \frac{1}{m} \sum_{i=0}^{m} \left[ -\frac{K}{2} \log(2\pi) - \frac{1}{2} \log |\Sigma| - \frac{1}{2} (X - f(z_i))^T \Sigma^{-1} (X - f(z_i)) \right] \\ &amp;\propto \frac{1}{m} \sum_{i=0}^{m} - \frac{1}{2} (X - f(z_i))^T \Sigma^{-1} (X - f(z_i)) \\ &amp;\propto \frac{1}{m} \sum_{i=0}^{m} \sum_{k=0}^{K} \frac{\left( x^{(k)} - f(z_i;\theta)^{(k)} \right)^{2}}{\sigma^{(k)}} \\ &amp;\propto \frac{1}{m} \sum_{i=0}^{m} \sum_{k=0}^{K} \left( x^{(k)} - f(z_i;\theta)^{(k)} \right)^{2} \end{aligned} \end{equation} $$因为真实后验分布 $p(z|X)$ 没有解析解，且KL散度这一项 $ D_{\text{KL}}(q_{\phi}(z|X) | p(z|X)) $ 始终是大于0的，因此目标优化函数可以改为最大化 第一项。在变分贝叶斯方法中，这个损失函数被称为变分下界或证据下界（variational lower bound, or evidence lower bound）">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-05-04T00:27:21+08:00">
    <meta property="article:modified_time" content="2025-05-04T00:27:21+08:00">

  <meta itemprop="name" content="Test">
  <meta itemprop="description" content="正文 注意这里可能会有歧义，
$$ \begin{equation} \begin{aligned} p_{\theta}(x) &amp;= \frac{p_{\theta}(X, z)}{p(z|X)} \\ &amp;= \frac{P_{\theta}(X|z)p(z)}{p(z|X)} \end{aligned} \end{equation} $$(注意这里可能会有歧义，我用 $p(z|X)$替代了 $p_{\theta}(z|X)$，但最后都会优化掉)。因为 $ p(z) \sim \mathcal{N}(z|0, I) $，而 $p_{\theta}(X|z)$ 就是Decoder所生成的分布，如果知道真实后验分布 $p(z|x)$，那我们也可以直接优化目标函数。但核心 $p(z|x)$ 是untracble的 （当然更严谨一点讲，也可以用hybird MC等方式来逼近，但就不在这里的讨论范畴了）。
于是在VAE中，我们可以用变分贝叶斯，引入一个Encoder，生成 $ q_{\phi}(z|X) \sim \mathcal{N}(z|\mu(X;\phi), \sigma(X;\phi)I)$ 来逼近真实后验分布 $p(z|X)$ （类似地，这里协方差矩阵也为对角矩阵）。重新推导目标函数
$$ \begin{equation} \begin{aligned} \log p_{\theta}(X) &amp;\approx \frac{1}{m} \sum_{i=0}^{m} p_{\theta}(X|z_{i}) \\ &amp;= \frac{1}{m} \sum_{i=0}^{m} \log \left( \frac{1}{(2\pi)^{K/2} |\Sigma|^{1/2}} \exp\left(-\frac{1}{2} (X - f(z_i))^T \Sigma^{-1} (X - f(z_i))\right) \right) \\ &amp;= \frac{1}{m} \sum_{i=0}^{m} \left[ -\frac{K}{2} \log(2\pi) - \frac{1}{2} \log |\Sigma| - \frac{1}{2} (X - f(z_i))^T \Sigma^{-1} (X - f(z_i)) \right] \\ &amp;\propto \frac{1}{m} \sum_{i=0}^{m} - \frac{1}{2} (X - f(z_i))^T \Sigma^{-1} (X - f(z_i)) \\ &amp;\propto \frac{1}{m} \sum_{i=0}^{m} \sum_{k=0}^{K} \frac{\left( x^{(k)} - f(z_i;\theta)^{(k)} \right)^{2}}{\sigma^{(k)}} \\ &amp;\propto \frac{1}{m} \sum_{i=0}^{m} \sum_{k=0}^{K} \left( x^{(k)} - f(z_i;\theta)^{(k)} \right)^{2} \end{aligned} \end{equation} $$因为真实后验分布 $p(z|X)$ 没有解析解，且KL散度这一项 $ D_{\text{KL}}(q_{\phi}(z|X) | p(z|X)) $ 始终是大于0的，因此目标优化函数可以改为最大化 第一项。在变分贝叶斯方法中，这个损失函数被称为变分下界或证据下界（variational lower bound, or evidence lower bound）">
  <meta itemprop="datePublished" content="2025-05-04T00:27:21+08:00">
  <meta itemprop="dateModified" content="2025-05-04T00:27:21+08:00">
  <meta itemprop="wordCount" content="274">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Test">
  <meta name="twitter:description" content="正文 注意这里可能会有歧义，
$$ \begin{equation} \begin{aligned} p_{\theta}(x) &amp;= \frac{p_{\theta}(X, z)}{p(z|X)} \\ &amp;= \frac{P_{\theta}(X|z)p(z)}{p(z|X)} \end{aligned} \end{equation} $$(注意这里可能会有歧义，我用 $p(z|X)$替代了 $p_{\theta}(z|X)$，但最后都会优化掉)。因为 $ p(z) \sim \mathcal{N}(z|0, I) $，而 $p_{\theta}(X|z)$ 就是Decoder所生成的分布，如果知道真实后验分布 $p(z|x)$，那我们也可以直接优化目标函数。但核心 $p(z|x)$ 是untracble的 （当然更严谨一点讲，也可以用hybird MC等方式来逼近，但就不在这里的讨论范畴了）。
于是在VAE中，我们可以用变分贝叶斯，引入一个Encoder，生成 $ q_{\phi}(z|X) \sim \mathcal{N}(z|\mu(X;\phi), \sigma(X;\phi)I)$ 来逼近真实后验分布 $p(z|X)$ （类似地，这里协方差矩阵也为对角矩阵）。重新推导目标函数
$$ \begin{equation} \begin{aligned} \log p_{\theta}(X) &amp;\approx \frac{1}{m} \sum_{i=0}^{m} p_{\theta}(X|z_{i}) \\ &amp;= \frac{1}{m} \sum_{i=0}^{m} \log \left( \frac{1}{(2\pi)^{K/2} |\Sigma|^{1/2}} \exp\left(-\frac{1}{2} (X - f(z_i))^T \Sigma^{-1} (X - f(z_i))\right) \right) \\ &amp;= \frac{1}{m} \sum_{i=0}^{m} \left[ -\frac{K}{2} \log(2\pi) - \frac{1}{2} \log |\Sigma| - \frac{1}{2} (X - f(z_i))^T \Sigma^{-1} (X - f(z_i)) \right] \\ &amp;\propto \frac{1}{m} \sum_{i=0}^{m} - \frac{1}{2} (X - f(z_i))^T \Sigma^{-1} (X - f(z_i)) \\ &amp;\propto \frac{1}{m} \sum_{i=0}^{m} \sum_{k=0}^{K} \frac{\left( x^{(k)} - f(z_i;\theta)^{(k)} \right)^{2}}{\sigma^{(k)}} \\ &amp;\propto \frac{1}{m} \sum_{i=0}^{m} \sum_{k=0}^{K} \left( x^{(k)} - f(z_i;\theta)^{(k)} \right)^{2} \end{aligned} \end{equation} $$因为真实后验分布 $p(z|X)$ 没有解析解，且KL散度这一项 $ D_{\text{KL}}(q_{\phi}(z|X) | p(z|X)) $ 始终是大于0的，因此目标优化函数可以改为最大化 第一项。在变分贝叶斯方法中，这个损失函数被称为变分下界或证据下界（variational lower bound, or evidence lower bound）">

	




<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  window.MathJax  = {
    tex: {
        inlineMath: [['$', '$'], ['\$', '\$']], 
        displayMath: [['$$', '$$']],
        processEnvironments: true,
        
        packages: ['base', 'ams', 'noerrors', 'noundefined'],
        tags: "ams",
    },
    loader:{
        load: ['ui/safe', '[tex]/ams'], 
    
    },
  };
</script>

<link rel="stylesheet" href="/css/custom.css">


  </head><body class="ma0 avenir bg-near-white development">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l center items-center justify-between">
    <a href="/" class="f3 fw2 hover-white white-90 dib no-underline">
      
        Renjie&#39;s log
      
    </a>
    <div class="flex-l items-center">
      

      
      <div class="ananke-socials"></div>

    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  
  <article class="flex-l mw8 center ph3 flex-wrap justify-between">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked ttu">
          
        Posts
      </aside><div id="sharing" class="mt3 ananke-socials"></div>
<h1 class="f1 athelas mt3 mb1">Test</h1>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2025-05-04T00:27:21+08:00">May 4, 2025</time>
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><h3 id="正文">正文</h3>
<p>注意这里可能会有歧义，</p>
$$
\begin{equation}
\begin{aligned}
p_{\theta}(x) &= \frac{p_{\theta}(X, z)}{p(z|X)} \\
    &= \frac{P_{\theta}(X|z)p(z)}{p(z|X)}
\end{aligned}
\end{equation}
$$<p>(注意这里可能会有歧义，我用 $p(z|X)$替代了 $p_{\theta}(z|X)$，但最后都会优化掉)。因为 $ p(z) \sim \mathcal{N}(z|0, I) $，而 $p_{\theta}(X|z)$ 就是Decoder所生成的分布，如果知道真实后验分布 $p(z|x)$，那我们也可以直接优化目标函数。但核心 $p(z|x)$ 是untracble的 （当然更严谨一点讲，也可以用hybird MC等方式来逼近，但就不在这里的讨论范畴了）。</p>
<p>于是在VAE中，我们可以用变分贝叶斯，引入一个Encoder，生成 $ q_{\phi}(z|X) \sim \mathcal{N}(z|\mu(X;\phi), \sigma(X;\phi)I)$ 来逼近真实后验分布 $p(z|X)$ （类似地，这里协方差矩阵也为对角矩阵）。重新推导目标函数</p>
$$
\begin{equation}
\begin{aligned}
\log p_{\theta}(X) &\approx \frac{1}{m} \sum_{i=0}^{m} p_{\theta}(X|z_{i}) \\
    &= \frac{1}{m} \sum_{i=0}^{m} \log \left( \frac{1}{(2\pi)^{K/2} |\Sigma|^{1/2}} \exp\left(-\frac{1}{2} (X - f(z_i))^T \Sigma^{-1} (X - f(z_i))\right) \right) \\
    &= \frac{1}{m} \sum_{i=0}^{m} \left[ -\frac{K}{2} \log(2\pi) - \frac{1}{2} \log |\Sigma| - \frac{1}{2} (X - f(z_i))^T \Sigma^{-1} (X - f(z_i)) \right] \\
    &\propto \frac{1}{m} \sum_{i=0}^{m} - \frac{1}{2} (X - f(z_i))^T \Sigma^{-1} (X - f(z_i)) \\ 
    &\propto \frac{1}{m} \sum_{i=0}^{m} \sum_{k=0}^{K} \frac{\left( x^{(k)} - f(z_i;\theta)^{(k)} \right)^{2}}{\sigma^{(k)}} \\
    &\propto \frac{1}{m} \sum_{i=0}^{m} \sum_{k=0}^{K} \left( x^{(k)} - f(z_i;\theta)^{(k)} \right)^{2}
\end{aligned}
\end{equation}
$$<p>因为真实后验分布 $p(z|X)$ 没有解析解，且KL散度这一项 $ D_{\text{KL}}(q_{\phi}(z|X) | p(z|X)) $ 始终是大于0的，因此目标优化函数可以改为最大化 第一项。在变分贝叶斯方法中，这个损失函数被称为<strong>变分下界或证据下界（variational lower bound, or evidence lower bound）</strong></p>
$$
\begin{equation}
\begin{aligned}
\log p_{\theta}(X) &\approx \frac{1}{m} \sum_{i=0}^{m} p_{\theta}(X|z_{i}) \\
    &= \frac{1}{m} \sum_{i=0}^{m} \log \left( \frac{1}{(2\pi)^{K/2} |\Sigma|^{1/2}} \exp\left(-\frac{1}{2} (X - f(z_i))^T \Sigma^{-1} (X - f(z_i))\right) \right) \\
    &= \frac{1}{m} \sum_{i=0}^{m} \left[ -\frac{K}{2} \log(2\pi) - \frac{1}{2} \log |\Sigma| - \frac{1}{2} (X - f(z_i))^T \Sigma^{-1} (X - f(z_i)) \right] \\
    &\propto \frac{1}{m} \sum_{i=0}^{m} - \frac{1}{2} (X - f(z_i))^T \Sigma^{-1} (X - f(z_i)) \\ 
    &\propto \frac{1}{m} \sum_{i=0}^{m} \sum_{k=0}^{K} \frac{\left( x^{(k)} - f(z_i;\theta)^{(k)} \right)^{2}}{\sigma^{(k)}} \\
    &\propto \frac{1}{m} \sum_{i=0}^{m} \sum_{k=0}^{K} \left( x^{(k)} - f(z_i;\theta)^{(k)} \right)^{2}
\end{aligned}
\end{equation}
$$<p>不难发现，最大化变分下界 等价于最大化 $p_{\theta}(x)$ 并最小化 $D_{\text{KL}}(q_{\phi}(z|X) | p(z|X))$，这2个目标都恰恰是我们希望优化的。那么来计算下该变分下界的解析解。</p>
<p>完美，第一项的解析解在前面我们已经算过了，通过MC采样我们可以近似求出其解析解，区别在于隐变量 $z$ 之前是从 $p(z) \sim \mathcal{N}(z|0, I) $ 中采样，而现在是从 $q_{\phi}(z|X) \sim \mathcal{N}(z|\mu(X;\phi), \sigma(X;\phi)I) $ 中采样；而第二项中，2个高斯分布间的KL散度也可以直接算出来解析解。</p>
<p>最后一步，让我们来计算优化目标最后的解析解。第一项，我们假设 $p_{\theta}(X|z)$ 的协方差为全为 $\frac{1}{2}$ 的对角矩阵；第二项，设隐变量 $z$ 维度为 $d$带入上述KL散度的解析解。优化目标的最终形式可以表示为。</p>
<p>好，得到目标损失函数了，可以训练VAE模型看看效果了，注意这里还是只对 $q_{\phi}(z|X) \sim \mathcal{N}(z|\mu(X;\phi), \sigma(X;\phi)I) $ 采样一次，因为我们引入Encoder构建约束关系后，采样出来的隐变量 $z$ 大部分都是有意义的，所以也不太担心会严重的模式坍塌现象了。类似的，在CelebA上，我训练了一个VAE模型，以下是损失曲线和生成图像的可视化结果：
<figure><img src="/pic_vae/vae_mse.png">
</figure>

<figure><img src="/pic_vae/vae_kld.png">
</figure>

<figure><img src="/pic_vae/vae_recons.png">
</figure>
</p>
<p>可以发现，无论是第一项的重建损失项还是第二项拟合真实后验分布的 $q_{\phi}(z|X)$ 和先验分布 $p(z)$ 间的KL散度，都是正常收敛的状态。可视化的结果也还不错，至少比较多样，不会崩塌到“平均脸”。但还有一个问题值得注意，因为我们优化的是变分下界，$D_{\text{KL}}(q_{\phi}(z|X) | p(z|X))$ 这一项可能会引入一些误差，但这个误差项又很难直观表现出来，这也是很多后续工作优化/讨论的地方。</p>
<p>其实关于VAE，还有很多值得讨论研究的点。比如最终优化目标的重建损失和KL散度，其实KL散度更像是正则项，重建损失和KL散度之间构成一个trade-off关系：更具体地来说，我们虽然会假设 $p_{\theta}(X|z)$ 的协方差矩阵为常数，但实际采样过程中，并不会再加一个高斯噪声来模拟，而是直接取均值，那如果这里对协方差矩阵的假设为一个与可控常数 $ \beta$ 相关的对角矩阵，最终优化目标可以（不严谨地）等价为以下形式：</p>
<p>那就等价于Beta-VAE了，当然Beta-VAE的出发点不同，是将等式重写为KKT条件下的拉格朗日形式。</p>
<p>VAE还有很多有意思的点，待后续学习&amp;更新吧。</p>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white white-70 dn dib-ns pv2 ph3 no-underline" href="http://localhost:1313/" >
    &copy;  Renjie's log 2025 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

  </body>
</html>
