<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep-Learning on Renjie&#39;s log</title>
    <link>http://localhost:1313/tags/deep-learning/</link>
    <description>Recent content in Deep-Learning on Renjie&#39;s log</description>
    <generator>Hugo</generator>
    <language>zh-cn</language>
    <lastBuildDate>Sun, 22 Jun 2025 13:43:59 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Diffusion-Based Generative Models &lt;1&gt;: DDPM</title>
      <link>http://localhost:1313/posts/diffusion-based-generative-models-1/</link>
      <pubDate>Sun, 22 Jun 2025 13:43:59 +0800</pubDate>
      <guid>http://localhost:1313/posts/diffusion-based-generative-models-1/</guid>
      <description>&lt;!-- summary = &#34;深入解读去噪扩散概率模型 (DDPM) 的核心算法与数学推导，揭示其如何通过前向加噪与反向去噪过程实现高质量生成。&#34; --&gt;&#xA;&lt;h2 id=&#34;一引言&#34;&gt;一、引言&lt;/h2&gt;&#xA;&lt;p&gt;扩散模型（&lt;strong&gt;Diffusion Models&lt;/strong&gt;）作为当前生成式 AI 的核心范式，受热力学启发&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，主要思想是迭代地加噪/去噪数据，模拟粒子扩散过程。在图像、视频生成等领域实现了非常好的效果。下文介绍核心代表作之一 &lt;strong&gt;DDPM&lt;/strong&gt; &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; (Denoising Diffusion Probabilistic Models)。&lt;/p&gt;&#xA;&lt;p&gt;随着 Diffusion-Based Generative Models 理论的逐渐完善，可以从多种视角（分数匹配、 微分方程等）推导出 DDPM 的前向/逆向扩散过程、优化目标和采样过程。这里，我们将遵循 DDPM 原文的思路进行推导。&lt;/p&gt;&#xA;&lt;h2 id=&#34;二ddpm-算法框架&#34;&gt;二、DDPM 算法框架&lt;/h2&gt;&#xA;&lt;h3 id=&#34;1-前向扩散过程forward-diffusion-process&#34;&gt;1. 前向扩散过程（Forward Diffusion Process）&lt;/h3&gt;&#xA;&lt;p&gt;前向扩散过程是&lt;em&gt;&lt;strong&gt;无参&lt;/strong&gt;&lt;/em&gt;的扩散过程，服从一个马尔可夫链 (Markov Chain)：马尔科夫链为状态空间中经过从一个状态到另一个状态的转换的随机过程，该过程要求具备&amp;quot;无记忆性&amp;quot;，即下一状态的概率分布只能由当前状态决定，在时间序列中它前面的事件均与之无关。&lt;/p&gt;&#xA;&lt;p&gt;具体来说，从一个真实数据分布采样 $\mathbf{x_0} \sim q(\mathbf{x})$，通过逐步对数据 $\mathbf{x_0}$ 添加高斯噪声（Gaussian Noise），得到被扰动的样本 $\mathbf{x_1},&amp;hellip;\mathbf{x_t},&amp;hellip;\mathbf{x_T}$，在 $T$ 步后接近纯噪声。得益于高斯分布的特殊数学性质，其线性组合仍然是高斯分布，因此可以将加噪过程中互相独立的高斯噪声进行合并:&lt;/p&gt;&#xA;$$&#xA;\begin{aligned}&#xA;\mathbf{x_t} &amp;= \sqrt{\alpha_t} \mathbf{x_{t-1}} + \sqrt{1 - \alpha_t} \epsilon_{t-1} \\&#xA;    &amp;= \sqrt{\alpha_t} \left( \sqrt{\alpha_{t-1}} \mathbf{x_{t-2}} + \sqrt{1 - \alpha_{t-1}} \epsilon_{t-2} \right) + \sqrt{1 - \alpha_t} \epsilon_{t-1} \\&#xA;    &amp;= \sqrt{\alpha_{t-1} \alpha_t} \mathbf{x_{t-2}} + \underbrace{{\sqrt{\alpha_t} \sqrt{1 - \alpha_{t-1}} \epsilon_{t-2} + \sqrt{1 - \alpha_t} \epsilon_{t-1}}}_{\text{Combine noise using linear Gaussian}} \\&#xA;    &amp;= \sqrt{\alpha_{t-1} \alpha_t} \mathbf{x_{t-2}} + \sqrt{1 - \alpha_{t-1} \alpha_t} \bar{\epsilon}_{t-2} \\&#xA;    &amp;=  ... \\&#xA;    &amp;= \sqrt{\bar{\alpha}_t} \mathbf{x_0} +  \sqrt{1 - \bar{\alpha}_t} \bar{\epsilon}_{t}&#xA;\end{aligned}&#xA;$$&lt;p&gt;其中 $ { \alpha_0, \dots, \alpha_T }$ 是一组人为设置的超参数，用于控制扩散过程中噪声的强度，定义 $\bar{\alpha_t} = \prod_{i=1}^t \alpha_i$。那么我们可以得到:&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
